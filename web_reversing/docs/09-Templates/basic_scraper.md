# åŸºç¡€çˆ¬è™«é¡¹ç›®æ¨¡æ¿

ä¸€ä¸ªç®€å•ä½†å®Œæ•´çš„ Python çˆ¬è™«é¡¹ç›®æ¨¡æ¿ï¼Œé€‚åˆå•æœºæ•°æ®é‡‡é›†ã€‚

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
basic_scraper/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ settings.py          # é…ç½®å‚æ•°
â”‚   â””â”€â”€ logging.conf         # æ—¥å¿—é…ç½®
â”œâ”€â”€ scrapers/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_scraper.py      # åŸºç¡€çˆ¬è™«ç±»
â”‚   â””â”€â”€ target_scraper.py    # ç›®æ ‡ç½‘ç«™çˆ¬è™«
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ crypto.py            # åŠ å¯†è§£å¯†å·¥å…·
â”‚   â”œâ”€â”€ headers.py           # è¯·æ±‚å¤´ç”Ÿæˆ
â”‚   â””â”€â”€ parser.py            # æ•°æ®è§£æ
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # åŸå§‹æ•°æ®
â”‚   â””â”€â”€ processed/           # å¤„ç†åæ•°æ®
â”œâ”€â”€ logs/                    # æ—¥å¿—ç›®å½•
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_scraper.py      # æµ‹è¯•ç”¨ä¾‹
â”œâ”€â”€ .env.example             # ç¯å¢ƒå˜é‡ç¤ºä¾‹
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ main.py                  # å…¥å£æ–‡ä»¶
```

---

## ğŸ“„ æ–‡ä»¶å†…å®¹

### 1. requirements.txt

```txt
requests>=2.31.0
beautifulsoup4>=4.12.0
lxml>=4.9.0
python-dotenv>=1.0.0
pycryptodome>=3.19.0
pymongo>=4.6.0
redis>=5.0.0
loguru>=0.7.0
```

### 2. config/settings.py

```python
"""
é¡¹ç›®é…ç½®æ–‡ä»¶
"""
import os
from dotenv import load_dotenv
from pathlib import Path

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é¡¹ç›®æ ¹ç›®å½•
BASE_DIR = Path(__file__).resolve().parent.parent

# ç›®æ ‡ç½‘ç«™é…ç½®
TARGET_URL = "https://example.com"
API_BASE_URL = "https://api.example.com"

# è¯·æ±‚é…ç½®
REQUEST_TIMEOUT = 30
MAX_RETRIES = 3
RETRY_DELAY = 2  # ç§’

# è¯·æ±‚å¤´é…ç½®
DEFAULT_HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Accept': 'application/json, text/plain, */*',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    'Accept-Encoding': 'gzip, deflate, br',
}

# ä»£ç†é…ç½®
USE_PROXY = os.getenv('USE_PROXY', 'false').lower() == 'true'
PROXY_URL = os.getenv('PROXY_URL', '')
PROXIES = {
    'http': PROXY_URL,
    'https': PROXY_URL,
} if USE_PROXY and PROXY_URL else None

# æ•°æ®åº“é…ç½®
MONGODB_URI = os.getenv('MONGODB_URI', 'mongodb://localhost:27017/')
MONGODB_DB = os.getenv('MONGODB_DB', 'scraper_db')
MONGODB_COLLECTION = os.getenv('MONGODB_COLLECTION', 'data')

# Redis é…ç½®
REDIS_HOST = os.getenv('REDIS_HOST', 'localhost')
REDIS_PORT = int(os.getenv('REDIS_PORT', 6379))
REDIS_DB = int(os.getenv('REDIS_DB', 0))
REDIS_PASSWORD = os.getenv('REDIS_PASSWORD', '')

# æ•°æ®å­˜å‚¨é…ç½®
DATA_DIR = BASE_DIR / 'data'
RAW_DATA_DIR = DATA_DIR / 'raw'
PROCESSED_DATA_DIR = DATA_DIR / 'processed'

# æ—¥å¿—é…ç½®
LOG_DIR = BASE_DIR / 'logs'
LOG_LEVEL = os.getenv('LOG_LEVEL', 'INFO')

# åˆ›å»ºå¿…è¦çš„ç›®å½•
for directory in [DATA_DIR, RAW_DATA_DIR, PROCESSED_DATA_DIR, LOG_DIR]:
    directory.mkdir(parents=True, exist_ok=True)

# ä¸šåŠ¡é…ç½®
BATCH_SIZE = 100  # æ‰¹å¤„ç†å¤§å°
SLEEP_INTERVAL = (1, 3)  # è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰èŒƒå›´
```

### 3. config/logging.conf

```python
"""
æ—¥å¿—é…ç½®
"""
from loguru import logger
import sys
from pathlib import Path

# æ—¥å¿—ç›®å½•
LOG_DIR = Path(__file__).resolve().parent.parent / 'logs'
LOG_DIR.mkdir(exist_ok=True)

# ç§»é™¤é»˜è®¤å¤„ç†å™¨
logger.remove()

# æ·»åŠ æ§åˆ¶å°è¾“å‡º
logger.add(
    sys.stdout,
    colorize=True,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>",
    level="INFO"
)

# æ·»åŠ æ–‡ä»¶è¾“å‡º - INFO çº§åˆ«
logger.add(
    LOG_DIR / "info_{time:YYYY-MM-DD}.log",
    rotation="00:00",  # æ¯å¤©åˆå¤œè½®è½¬
    retention="30 days",  # ä¿ç•™30å¤©
    encoding="utf-8",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}",
    level="INFO"
)

# æ·»åŠ æ–‡ä»¶è¾“å‡º - ERROR çº§åˆ«
logger.add(
    LOG_DIR / "error_{time:YYYY-MM-DD}.log",
    rotation="00:00",
    retention="90 days",  # é”™è¯¯æ—¥å¿—ä¿ç•™90å¤©
    encoding="utf-8",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}",
    level="ERROR"
)
```

### 4. scrapers/base_scraper.py

```python
"""
åŸºç¡€çˆ¬è™«ç±»
"""
import time
import random
import requests
from typing import Dict, Any, Optional
from config.settings import *
from config.logging import logger


class BaseScraper:
    """åŸºç¡€çˆ¬è™«ç±»"""

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update(DEFAULT_HEADERS)

    def request(
        self,
        method: str,
        url: str,
        **kwargs
    ) -> Optional[requests.Response]:
        """
        å‘é€ HTTP è¯·æ±‚ï¼ˆå¸¦é‡è¯•ï¼‰

        Args:
            method: HTTP æ–¹æ³• (GET, POST, etc.)
            url: è¯·æ±‚ URL
            **kwargs: å…¶ä»–è¯·æ±‚å‚æ•°

        Returns:
            Response å¯¹è±¡æˆ– None
        """
        for attempt in range(MAX_RETRIES):
            try:
                # åˆå¹¶ä»£ç†é…ç½®
                if PROXIES:
                    kwargs.setdefault('proxies', PROXIES)

                # è®¾ç½®è¶…æ—¶
                kwargs.setdefault('timeout', REQUEST_TIMEOUT)

                # å‘é€è¯·æ±‚
                response = self.session.request(method, url, **kwargs)
                response.raise_for_status()

                logger.info(f"è¯·æ±‚æˆåŠŸ: {method} {url} - {response.status_code}")
                return response

            except requests.exceptions.RequestException as e:
                logger.warning(f"è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{MAX_RETRIES}): {e}")

                if attempt < MAX_RETRIES - 1:
                    time.sleep(RETRY_DELAY * (attempt + 1))
                else:
                    logger.error(f"è¯·æ±‚æœ€ç»ˆå¤±è´¥: {method} {url}")
                    return None

    def get(self, url: str, **kwargs) -> Optional[requests.Response]:
        """GET è¯·æ±‚"""
        return self.request('GET', url, **kwargs)

    def post(self, url: str, **kwargs) -> Optional[requests.Response]:
        """POST è¯·æ±‚"""
        return self.request('POST', url, **kwargs)

    def sleep(self):
        """éšæœºä¼‘çœ """
        sleep_time = random.uniform(*SLEEP_INTERVAL)
        logger.debug(f"ä¼‘çœ  {sleep_time:.2f} ç§’")
        time.sleep(sleep_time)

    def save_json(self, data: Dict[Any, Any], filename: str):
        """ä¿å­˜ JSON æ•°æ®"""
        import json
        filepath = PROCESSED_DATA_DIR / filename

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        logger.info(f"æ•°æ®å·²ä¿å­˜: {filepath}")

    def save_csv(self, data: list, filename: str):
        """ä¿å­˜ CSV æ•°æ®"""
        import csv

        if not data:
            logger.warning("æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return

        filepath = PROCESSED_DATA_DIR / filename

        with open(filepath, 'w', encoding='utf-8', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=data[0].keys())
            writer.writeheader()
            writer.writerows(data)

        logger.info(f"æ•°æ®å·²ä¿å­˜: {filepath} ({len(data)} æ¡)")
```

### 5. scrapers/target_scraper.py

```python
"""
ç›®æ ‡ç½‘ç«™çˆ¬è™«
"""
from bs4 import BeautifulSoup
from .base_scraper import BaseScraper
from config.settings import TARGET_URL
from config.logging import logger


class TargetScraper(BaseScraper):
    """ç›®æ ‡ç½‘ç«™çˆ¬è™«"""

    def __init__(self):
        super().__init__()
        self.base_url = TARGET_URL

    def scrape_list(self, page: int = 1):
        """
        æŠ“å–åˆ—è¡¨é¡µ

        Args:
            page: é¡µç 

        Returns:
            åˆ—è¡¨æ•°æ®
        """
        url = f"{self.base_url}/list?page={page}"
        response = self.get(url)

        if not response:
            return []

        soup = BeautifulSoup(response.text, 'lxml')
        items = []

        # TODO: å®ç°å…·ä½“çš„è§£æé€»è¾‘
        for item in soup.select('.item'):
            data = {
                'title': item.select_one('.title').text.strip(),
                'link': item.select_one('a')['href'],
                # æ·»åŠ æ›´å¤šå­—æ®µ...
            }
            items.append(data)

        logger.info(f"é¡µé¢ {page} æŠ“å–åˆ° {len(items)} æ¡æ•°æ®")
        return items

    def scrape_detail(self, url: str):
        """
        æŠ“å–è¯¦æƒ…é¡µ

        Args:
            url: è¯¦æƒ…é¡µ URL

        Returns:
            è¯¦æƒ…æ•°æ®
        """
        response = self.get(url)

        if not response:
            return None

        soup = BeautifulSoup(response.text, 'lxml')

        # TODO: å®ç°å…·ä½“çš„è§£æé€»è¾‘
        data = {
            'url': url,
            'title': soup.select_one('h1').text.strip(),
            'content': soup.select_one('.content').text.strip(),
            # æ·»åŠ æ›´å¤šå­—æ®µ...
        }

        logger.info(f"è¯¦æƒ…é¡µæŠ“å–æˆåŠŸ: {url}")
        return data

    def run(self, start_page: int = 1, end_page: int = 10):
        """
        è¿è¡Œçˆ¬è™«

        Args:
            start_page: èµ·å§‹é¡µ
            end_page: ç»“æŸé¡µ
        """
        logger.info(f"å¼€å§‹æŠ“å–: é¡µé¢ {start_page} åˆ° {end_page}")

        all_items = []

        for page in range(start_page, end_page + 1):
            # æŠ“å–åˆ—è¡¨
            items = self.scrape_list(page)
            all_items.extend(items)

            # ä¼‘çœ 
            self.sleep()

        # ä¿å­˜æ•°æ®
        self.save_json(all_items, 'list_data.json')
        self.save_csv(all_items, 'list_data.csv')

        logger.info(f"æŠ“å–å®Œæˆ: å…± {len(all_items)} æ¡æ•°æ®")
```

### 6. main.py

```python
"""
ä¸»ç¨‹åºå…¥å£
"""
from scrapers.target_scraper import TargetScraper
from config.logging import logger


def main():
    """ä¸»å‡½æ•°"""
    logger.info("=" * 50)
    logger.info("çˆ¬è™«ç¨‹åºå¯åŠ¨")
    logger.info("=" * 50)

    try:
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        scraper = TargetScraper()

        # è¿è¡Œçˆ¬è™«
        scraper.run(start_page=1, end_page=5)

        logger.info("=" * 50)
        logger.info("çˆ¬è™«ç¨‹åºç»“æŸ")
        logger.info("=" * 50)

    except KeyboardInterrupt:
        logger.warning("ç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        logger.exception(f"ç¨‹åºå¼‚å¸¸: {e}")


if __name__ == '__main__':
    main()
```

### 7. .env.example

```bash
# ä»£ç†é…ç½®
USE_PROXY=false
PROXY_URL=http://127.0.0.1:7890

# æ•°æ®åº“é…ç½®
MONGODB_URI=mongodb://localhost:27017/
MONGODB_DB=scraper_db
MONGODB_COLLECTION=data

# Redis é…ç½®
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=

# æ—¥å¿—çº§åˆ«
LOG_LEVEL=INFO
```

### 8. .gitignore

```
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
venv/
ENV/
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# ç¯å¢ƒå˜é‡
.env
.env.local

# æ•°æ®å’Œæ—¥å¿—
data/
logs/
*.log

# IDE
.vscode/
.idea/
*.swp
*.swo
.DS_Store

# æµ‹è¯•
.pytest_cache/
.coverage
htmlcov/
```

### 9. README.md

````markdown
# åŸºç¡€çˆ¬è™«é¡¹ç›®

## åŠŸèƒ½ç‰¹æ€§

- âœ… å®Œæ•´çš„é¡¹ç›®ç»“æ„
- âœ… é…ç½®æ–‡ä»¶ç®¡ç†
- âœ… æ—¥å¿—ç³»ç»Ÿ (Loguru)
- âœ… é”™è¯¯é‡è¯•æœºåˆ¶
- âœ… ä»£ç†æ”¯æŒ
- âœ… æ•°æ®å­˜å‚¨ (JSON/CSV)
- âœ… ç¯å¢ƒå˜é‡é…ç½®

## å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

\`\`\`bash
pip install -r requirements.txt
\`\`\`

### 2. é…ç½®ç¯å¢ƒ

\`\`\`bash
cp .env.example .env

# ç¼–è¾‘ .env æ–‡ä»¶

\`\`\`

### 3. è¿è¡Œçˆ¬è™«

\`\`\`bash
python main.py
\`\`\`

## é¡¹ç›®ç»“æ„

\`\`\`
.
â”œâ”€â”€ config/ # é…ç½®æ–‡ä»¶
â”œâ”€â”€ scrapers/ # çˆ¬è™«æ¨¡å—
â”œâ”€â”€ utils/ # å·¥å…·å‡½æ•°
â”œâ”€â”€ data/ # æ•°æ®å­˜å‚¨
â”œâ”€â”€ logs/ # æ—¥å¿—æ–‡ä»¶
â””â”€â”€ main.py # å…¥å£æ–‡ä»¶
\`\`\`

## ä½¿ç”¨è¯´æ˜

### è‡ªå®šä¹‰çˆ¬è™«

1. ç¼–è¾‘ `scrapers/target_scraper.py`
2. å®ç° `scrape_list()` å’Œ `scrape_detail()` æ–¹æ³•
3. ä¿®æ”¹ `config/settings.py` ä¸­çš„ `TARGET_URL`

### æ•°æ®å­˜å‚¨

æ•°æ®é»˜è®¤ä¿å­˜åœ¨ `data/processed/` ç›®å½•ï¼š

- JSON æ ¼å¼: `list_data.json`
- CSV æ ¼å¼: `list_data.csv`

### æ—¥å¿—æŸ¥çœ‹

æ—¥å¿—ä¿å­˜åœ¨ `logs/` ç›®å½•ï¼š

- INFO æ—¥å¿—: `info_YYYY-MM-DD.log`
- ERROR æ—¥å¿—: `error_YYYY-MM-DD.log`

## License

MIT
\`\`\`

---

## ğŸš€ ä½¿ç”¨æ­¤æ¨¡æ¿

```bash
# 1. å¤åˆ¶æ¨¡æ¿
cp -r templates/basic_scraper my_project
cd my_project

# 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or
venv\Scripts\activate  # Windows

# 3. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 4. é…ç½®ç¯å¢ƒ
cp .env.example .env
vim .env

# 5. è¿è¡Œ
python main.py
```
````

---

## ğŸ“š ç›¸å…³æ¨¡æ¿

- [é€†å‘åˆ†æé¡¹ç›®](./reverse_project.md)
- [Docker éƒ¨ç½²](./docker_setup.md)
- [åˆ†å¸ƒå¼çˆ¬è™«](./distributed_crawler.md)
