# åˆ†å¸ƒå¼çˆ¬è™«æ¶æ„æ¨¡æ¿

åŸºäº Scrapy + Redis çš„åˆ†å¸ƒå¼çˆ¬è™«æ¶æ„ï¼Œæ”¯æŒå¤šæœºååŒã€ä»»åŠ¡è°ƒåº¦å’Œæ•°æ®å»é‡ã€‚

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
distributed_crawler/
â”œâ”€â”€ scrapy_project/
â”‚   â”œâ”€â”€ scrapy.cfg
â”‚   â”œâ”€â”€ spiders/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_spider.py
â”‚   â”‚   â””â”€â”€ target_spider.py
â”‚   â”œâ”€â”€ items.py
â”‚   â”œâ”€â”€ pipelines.py
â”‚   â”œâ”€â”€ middlewares.py
â”‚   â”œâ”€â”€ settings.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ redis_client.py
â”‚       â””â”€â”€ bloomfilter.py
â”œâ”€â”€ scheduler/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ task_manager.py
â”‚   â””â”€â”€ url_scheduler.py
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ settings.py
â”‚   â””â”€â”€ redis_config.py
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile.spider
â”‚   â”œâ”€â”€ Dockerfile.scheduler
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ start_spider.sh
â”‚   â”œâ”€â”€ add_tasks.py
â”‚   â””â”€â”€ monitor.py
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_spider.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ“„ æ ¸å¿ƒæ–‡ä»¶

### 1. scrapy_project/settings.py

```python
"""
Scrapy åˆ†å¸ƒå¼é…ç½®
"""
import os
from dotenv import load_dotenv

load_dotenv()

# Scrapy åŸºç¡€é…ç½®
BOT_NAME = 'distributed_crawler'
SPIDER_MODULES = ['scrapy_project.spiders']
NEWSPIDER_MODULE = 'scrapy_project.spiders'

# éµå®ˆ robots.txt
ROBOTSTXT_OBEY = False

# å¹¶å‘é…ç½®
CONCURRENT_REQUESTS = 16
CONCURRENT_REQUESTS_PER_DOMAIN = 8
CONCURRENT_REQUESTS_PER_IP = 8

# ä¸‹è½½å»¶è¿Ÿ
DOWNLOAD_DELAY = 1
RANDOMIZE_DOWNLOAD_DELAY = True

# è¶…æ—¶è®¾ç½®
DOWNLOAD_TIMEOUT = 30
RETRY_TIMES = 3
RETRY_HTTP_CODES = [500, 502, 503, 504, 408, 429]

# User-Agent
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'

# è¯·æ±‚å¤´
DEFAULT_REQUEST_HEADERS = {
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    'Accept-Encoding': 'gzip, deflate',
}

# ==================== Redis é…ç½® ====================

# ä½¿ç”¨ scrapy-redis
SCHEDULER = "scrapy_redis.scheduler.Scheduler"
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
SCHEDULER_PERSIST = True  # ä¿æŒè°ƒåº¦é˜Ÿåˆ—

# Redis è¿æ¥
REDIS_HOST = os.getenv('REDIS_HOST', 'localhost')
REDIS_PORT = int(os.getenv('REDIS_PORT', 6379))
REDIS_PASSWORD = os.getenv('REDIS_PASSWORD', '')
REDIS_DB = int(os.getenv('REDIS_DB', 0))

# Redis è¿æ¥é…ç½®
REDIS_PARAMS = {
    'password': REDIS_PASSWORD,
    'db': REDIS_DB,
    'decode_responses': False,  # å…³é”®ï¼šä¸è‡ªåŠ¨è§£ç 
}

# è°ƒåº¦é˜Ÿåˆ— key å‰ç¼€
SCHEDULER_QUEUE_KEY = '%(spider)s:requests'
SCHEDULER_DUPEFILTER_KEY = '%(spider)s:dupefilter'

# ==================== ä¸­é—´ä»¶ ====================

DOWNLOADER_MIDDLEWARES = {
    # é‡è¯•ä¸­é—´ä»¶
    'scrapy.downloadermiddlewares.retry.RetryMiddleware': 90,

    # Redis ç»Ÿè®¡ä¸­é—´ä»¶
    'scrapy_redis.downloadermiddleware.RedisStatsMiddleware': 100,

    # éšæœº User-Agent
    'scrapy_project.middlewares.RandomUserAgentMiddleware': 400,

    # ä»£ç†ä¸­é—´ä»¶
    'scrapy_project.middlewares.ProxyMiddleware': 410,

    # å¼‚å¸¸æ•è·
    'scrapy_project.middlewares.ExceptionMiddleware': 500,
}

SPIDER_MIDDLEWARES = {
    'scrapy_redis.spidermiddleware.RedisSrpiderMiddleware': 100,
}

# ==================== Pipeline ====================

ITEM_PIPELINES = {
    # æ•°æ®æ¸…æ´—
    'scrapy_project.pipelines.CleanPipeline': 100,

    # å»é‡æ£€æŸ¥
    'scrapy_project.pipelines.DuplicatesPipeline': 200,

    # MongoDB å­˜å‚¨
    'scrapy_project.pipelines.MongoPipeline': 300,

    # Redis ç¼“å­˜
    'scrapy_project.pipelines.RedisPipeline': 400,
}

# ==================== MongoDB é…ç½® ====================

MONGODB_URI = os.getenv('MONGODB_URI', 'mongodb://localhost:27017/')
MONGODB_DATABASE = os.getenv('MONGODB_DB', 'scraper_db')
MONGODB_COLLECTION = '%(spider)s_items'

# ==================== æ—¥å¿—é…ç½® ====================

LOG_LEVEL = 'INFO'
LOG_FORMAT = '%(asctime)s [%(name)s] %(levelname)s: %(message)s'
LOG_DATEFORMAT = '%Y-%m-%d %H:%M:%S'

# ==================== å…¶ä»–é…ç½® ====================

# è‡ªåŠ¨é™é€Ÿ
AUTOTHROTTLE_ENABLED = True
AUTOTHROTTLE_START_DELAY = 1
AUTOTHROTTLE_MAX_DELAY = 10
AUTOTHROTTLE_TARGET_CONCURRENCY = 2.0

# Cookies
COOKIES_ENABLED = True
COOKIES_DEBUG = False

# Telnet æ§åˆ¶å°
TELNETCONSOLE_ENABLED = True
TELNETCONSOLE_PORT = [6023, 6024, 6025]
```

### 2. scrapy_project/items.py

```python
"""
æ•°æ®æ¨¡å‹å®šä¹‰
"""
import scrapy
from scrapy.loader.processors import TakeFirst, MapCompose, Join
from w3lib.html import remove_tags


def clean_text(text):
    """æ¸…ç†æ–‡æœ¬"""
    return text.strip() if text else ''


class BaseItem(scrapy.Item):
    """åŸºç¡€ Item"""
    # é€šç”¨å­—æ®µ
    url = scrapy.Field(output_processor=TakeFirst())
    spider_name = scrapy.Field(output_processor=TakeFirst())
    crawl_time = scrapy.Field(output_processor=TakeFirst())


class ProductItem(BaseItem):
    """å•†å“ Item"""
    title = scrapy.Field(
        input_processor=MapCompose(remove_tags, clean_text),
        output_processor=TakeFirst()
    )
    price = scrapy.Field(output_processor=TakeFirst())
    stock = scrapy.Field(output_processor=TakeFirst())
    description = scrapy.Field(
        input_processor=MapCompose(remove_tags, clean_text),
        output_processor=Join('\n')
    )
    images = scrapy.Field()
    tags = scrapy.Field()
    rating = scrapy.Field(output_processor=TakeFirst())
    reviews_count = scrapy.Field(output_processor=TakeFirst())


class ArticleItem(BaseItem):
    """æ–‡ç«  Item"""
    title = scrapy.Field(
        input_processor=MapCompose(remove_tags, clean_text),
        output_processor=TakeFirst()
    )
    author = scrapy.Field(output_processor=TakeFirst())
    publish_time = scrapy.Field(output_processor=TakeFirst())
    content = scrapy.Field(
        input_processor=MapCompose(remove_tags, clean_text),
        output_processor=Join('\n')
    )
    category = scrapy.Field()
    tags = scrapy.Field()
    view_count = scrapy.Field(output_processor=TakeFirst())
```

### 3. scrapy_project/pipelines.py

```python
"""
æ•°æ®å¤„ç† Pipeline
"""
import hashlib
import pymongo
import redis
from datetime import datetime
from itemadapter import ItemAdapter
from scrapy.exceptions import DropItem


class CleanPipeline:
    """æ•°æ®æ¸…æ´— Pipeline"""

    def process_item(self, item, spider):
        adapter = ItemAdapter(item)

        # æ·»åŠ çˆ¬å–æ—¶é—´
        adapter['crawl_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        adapter['spider_name'] = spider.name

        # æ¸…ç†ç©ºå€¼
        for field in adapter.field_names():
            value = adapter.get(field)
            if value is None or value == '':
                adapter[field] = None

        return item


class DuplicatesPipeline:
    """å»é‡ Pipeline (åŸºäº Redis)"""

    def __init__(self, redis_host, redis_port, redis_password, redis_db):
        self.redis_host = redis_host
        self.redis_port = redis_port
        self.redis_password = redis_password
        self.redis_db = redis_db
        self.redis_client = None

    @classmethod
    def from_crawler(cls, crawler):
        return cls(
            redis_host=crawler.settings.get('REDIS_HOST'),
            redis_port=crawler.settings.get('REDIS_PORT'),
            redis_password=crawler.settings.get('REDIS_PASSWORD'),
            redis_db=crawler.settings.get('REDIS_DB'),
        )

    def open_spider(self, spider):
        self.redis_client = redis.Redis(
            host=self.redis_host,
            port=self.redis_port,
            password=self.redis_password,
            db=self.redis_db,
            decode_responses=True
        )

    def close_spider(self, spider):
        if self.redis_client:
            self.redis_client.close()

    def process_item(self, item, spider):
        adapter = ItemAdapter(item)

        # ç”Ÿæˆå”¯ä¸€æ ‡è¯†
        url = adapter.get('url', '')
        item_hash = hashlib.md5(url.encode()).hexdigest()

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        key = f"{spider.name}:items:{item_hash}"
        if self.redis_client.exists(key):
            raise DropItem(f"Duplicate item found: {url}")

        # æ ‡è®°å·²å¤„ç†
        self.redis_client.setex(key, 86400 * 7, '1')  # ä¿ç•™7å¤©

        return item


class MongoPipeline:
    """MongoDB å­˜å‚¨ Pipeline"""

    def __init__(self, mongo_uri, mongo_db):
        self.mongo_uri = mongo_uri
        self.mongo_db = mongo_db
        self.client = None
        self.db = None

    @classmethod
    def from_crawler(cls, crawler):
        return cls(
            mongo_uri=crawler.settings.get('MONGODB_URI'),
            mongo_db=crawler.settings.get('MONGODB_DATABASE')
        )

    def open_spider(self, spider):
        self.client = pymongo.MongoClient(self.mongo_uri)
        self.db = self.client[self.mongo_db]

    def close_spider(self, spider):
        if self.client:
            self.client.close()

    def process_item(self, item, spider):
        collection_name = f"{spider.name}_items"
        collection = self.db[collection_name]

        # æ’å…¥æ•°æ®
        item_dict = ItemAdapter(item).asdict()
        collection.insert_one(item_dict)

        spider.logger.info(f"Item saved to MongoDB: {item_dict.get('url')}")
        return item


class RedisPipeline:
    """Redis ç¼“å­˜ Pipeline"""

    def __init__(self, redis_host, redis_port, redis_password, redis_db):
        self.redis_host = redis_host
        self.redis_port = redis_port
        self.redis_password = redis_password
        self.redis_db = redis_db
        self.redis_client = None

    @classmethod
    def from_crawler(cls, crawler):
        return cls(
            redis_host=crawler.settings.get('REDIS_HOST'),
            redis_port=crawler.settings.get('REDIS_PORT'),
            redis_password=crawler.settings.get('REDIS_PASSWORD'),
            redis_db=crawler.settings.get('REDIS_DB') + 1,  # ä½¿ç”¨ä¸åŒçš„ DB
        )

    def open_spider(self, spider):
        self.redis_client = redis.Redis(
            host=self.redis_host,
            port=self.redis_port,
            password=self.redis_password,
            db=self.redis_db,
            decode_responses=True
        )

    def close_spider(self, spider):
        if self.redis_client:
            self.redis_client.close()

    def process_item(self, item, spider):
        import json

        adapter = ItemAdapter(item)
        url = adapter.get('url', '')
        item_hash = hashlib.md5(url.encode()).hexdigest()

        # ç¼“å­˜åˆ° Redis
        key = f"{spider.name}:cache:{item_hash}"
        value = json.dumps(adapter.asdict(), ensure_ascii=False)
        self.redis_client.setex(key, 3600, value)  # ç¼“å­˜1å°æ—¶

        return item
```

### 4. scrapy_project/middlewares.py

```python
"""
ä¸­é—´ä»¶
"""
import random
from scrapy import signals
from scrapy.downloadermiddlewares.useragent import UserAgentMiddleware
from scrapy.exceptions import IgnoreRequest


class RandomUserAgentMiddleware(UserAgentMiddleware):
    """éšæœº User-Agent ä¸­é—´ä»¶"""

    USER_AGENTS = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 14_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    ]

    def process_request(self, request, spider):
        user_agent = random.choice(self.USER_AGENTS)
        request.headers['User-Agent'] = user_agent


class ProxyMiddleware:
    """ä»£ç†ä¸­é—´ä»¶"""

    def __init__(self, proxy_list):
        self.proxy_list = proxy_list

    @classmethod
    def from_crawler(cls, crawler):
        # ä»é…ç½®æˆ– Redis è·å–ä»£ç†åˆ—è¡¨
        proxy_list = crawler.settings.get('PROXY_LIST', [])
        return cls(proxy_list)

    def process_request(self, request, spider):
        if self.proxy_list:
            proxy = random.choice(self.proxy_list)
            request.meta['proxy'] = proxy
            spider.logger.debug(f"Using proxy: {proxy}")

    def process_exception(self, request, exception, spider):
        # ä»£ç†å¤±è´¥æ—¶ï¼Œç§»é™¤è¯¥ä»£ç†
        proxy = request.meta.get('proxy')
        if proxy and proxy in self.proxy_list:
            self.proxy_list.remove(proxy)
            spider.logger.warning(f"Removed failed proxy: {proxy}")


class ExceptionMiddleware:
    """å¼‚å¸¸å¤„ç†ä¸­é—´ä»¶"""

    def process_response(self, request, response, spider):
        # æ£€æŸ¥å“åº”çŠ¶æ€ç 
        if response.status in [403, 429]:
            spider.logger.warning(f"Rate limited or blocked: {response.url}")
            # å¯ä»¥åœ¨è¿™é‡Œå®ç°å»¶è¿Ÿé‡è¯•é€»è¾‘

        return response

    def process_exception(self, request, exception, spider):
        spider.logger.error(f"Request failed: {request.url} - {exception}")
```

### 5. scrapy_project/spiders/base_spider.py

```python
"""
åŸºç¡€çˆ¬è™«ç±»
"""
from scrapy_redis.spiders import RedisSpider
from scrapy.loader import ItemLoader


class BaseRedisSpider(RedisSpider):
    """åŸºç¡€ Redis çˆ¬è™«"""

    # Redis key (éœ€è¦åœ¨å­ç±»ä¸­è¦†ç›–)
    redis_key = None

    # æ‰¹å¤„ç†å¤§å°
    redis_batch_size = 10

    # æœ€å¤§ç©ºé—²æ—¶é—´ (ç§’)
    max_idle_time = 60

    def make_request_from_data(self, data):
        """
        ä» Redis è·å–çš„æ•°æ®åˆ›å»ºè¯·æ±‚

        Args:
            data: ä» Redis è·å–çš„ URL æˆ–æ•°æ®

        Returns:
            Request å¯¹è±¡
        """
        url = data.decode('utf-8') if isinstance(data, bytes) else data
        return self.make_requests_from_url(url)

    def parse(self, response):
        """éœ€è¦åœ¨å­ç±»ä¸­å®ç°"""
        raise NotImplementedError

    def get_item_loader(self, item_class, response):
        """åˆ›å»º ItemLoader"""
        loader = ItemLoader(item=item_class(), response=response)
        loader.add_value('url', response.url)
        return loader
```

### 6. scrapy_project/spiders/target_spider.py

```python
"""
ç›®æ ‡ç½‘ç«™çˆ¬è™«
"""
from scrapy import Request
from .base_spider import BaseRedisSpider
from scrapy_project.items import ProductItem


class TargetSpider(BaseRedisSpider):
    """ç›®æ ‡ç½‘ç«™çˆ¬è™«"""

    name = 'target_spider'
    redis_key = 'target_spider:start_urls'

    custom_settings = {
        'CONCURRENT_REQUESTS': 8,
        'DOWNLOAD_DELAY': 2,
    }

    def parse(self, response):
        """
        è§£æåˆ—è¡¨é¡µ
        """
        # æå–äº§å“é“¾æ¥
        for href in response.css('.product-item a::attr(href)').getall():
            url = response.urljoin(href)
            yield Request(url, callback=self.parse_detail)

        # ç¿»é¡µ
        next_page = response.css('.pagination .next::attr(href)').get()
        if next_page:
            yield Request(response.urljoin(next_page), callback=self.parse)

    def parse_detail(self, response):
        """
        è§£æè¯¦æƒ…é¡µ
        """
        loader = self.get_item_loader(ProductItem, response)

        # æå–å­—æ®µ
        loader.add_css('title', 'h1.product-title::text')
        loader.add_css('price', '.price::text')
        loader.add_css('stock', '.stock::text')
        loader.add_css('description', '.description *::text')
        loader.add_css('images', '.product-images img::attr(src)')
        loader.add_css('tags', '.tags a::text')
        loader.add_css('rating', '.rating::attr(data-rating)')
        loader.add_css('reviews_count', '.reviews-count::text')

        yield loader.load_item()
```

### 7. scheduler/task_manager.py

```python
"""
ä»»åŠ¡ç®¡ç†å™¨
"""
import redis
import logging


class TaskManager:
    """åˆ†å¸ƒå¼ä»»åŠ¡ç®¡ç†å™¨"""

    def __init__(self, redis_host='localhost', redis_port=6379, redis_password='', redis_db=0):
        self.redis_client = redis.Redis(
            host=redis_host,
            port=redis_port,
            password=redis_password,
            db=redis_db,
            decode_responses=True
        )
        self.logger = logging.getLogger(__name__)

    def add_start_urls(self, spider_name, urls):
        """
        æ·»åŠ èµ·å§‹ URL åˆ° Redis

        Args:
            spider_name: çˆ¬è™«åç§°
            urls: URL åˆ—è¡¨
        """
        key = f"{spider_name}:start_urls"

        if isinstance(urls, str):
            urls = [urls]

        added = self.redis_client.lpush(key, *urls)
        self.logger.info(f"Added {added} URLs to {key}")
        return added

    def get_queue_size(self, spider_name):
        """è·å–é˜Ÿåˆ—å¤§å°"""
        key = f"{spider_name}:start_urls"
        return self.redis_client.llen(key)

    def clear_queue(self, spider_name):
        """æ¸…ç©ºé˜Ÿåˆ—"""
        key = f"{spider_name}:start_urls"
        deleted = self.redis_client.delete(key)
        self.logger.info(f"Cleared queue: {key}")
        return deleted

    def get_stats(self, spider_name):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'queue_size': self.get_queue_size(spider_name),
            'items_count': self.redis_client.scard(f"{spider_name}:dupefilter"),
        }
        return stats


if __name__ == '__main__':
    # ä½¿ç”¨ç¤ºä¾‹
    logging.basicConfig(level=logging.INFO)

    manager = TaskManager(
        redis_host='localhost',
        redis_port=6379,
        redis_password='redis123'
    )

    # æ·»åŠ èµ·å§‹ URL
    urls = [
        'https://example.com/page/1',
        'https://example.com/page/2',
        'https://example.com/page/3',
    ]

    manager.add_start_urls('target_spider', urls)

    # æŸ¥çœ‹ç»Ÿè®¡
    stats = manager.get_stats('target_spider')
    print(f"Stats: {stats}")
```

### 8. scripts/add_tasks.py

```python
"""
æ‰¹é‡æ·»åŠ ä»»åŠ¡è„šæœ¬
"""
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

from scheduler.task_manager import TaskManager
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s'
)


def generate_urls(base_url, start_page, end_page):
    """ç”Ÿæˆ URL åˆ—è¡¨"""
    return [f"{base_url}?page={i}" for i in range(start_page, end_page + 1)]


def main():
    manager = TaskManager(
        redis_host='localhost',
        redis_port=6379,
        redis_password='redis123'
    )

    # ç”Ÿæˆ URL
    urls = generate_urls(
        base_url='https://example.com/products',
        start_page=1,
        end_page=100
    )

    # æ·»åŠ åˆ°é˜Ÿåˆ—
    manager.add_start_urls('target_spider', urls)

    # æŸ¥çœ‹ç»Ÿè®¡
    stats = manager.get_stats('target_spider')
    print(f"\nâœ… Tasks added successfully!")
    print(f"ğŸ“Š Queue size: {stats['queue_size']}")


if __name__ == '__main__':
    main()
```

### 9. scripts/monitor.py

```python
"""
çˆ¬è™«ç›‘æ§è„šæœ¬
"""
import sys
import os
import time
import redis

sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))


class CrawlerMonitor:
    """çˆ¬è™«ç›‘æ§"""

    def __init__(self, redis_host='localhost', redis_port=6379, redis_password='', redis_db=0):
        self.redis_client = redis.Redis(
            host=redis_host,
            port=redis_port,
            password=redis_password,
            db=redis_db,
            decode_responses=True
        )

    def get_spider_stats(self, spider_name):
        """è·å–çˆ¬è™«ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}

        # é˜Ÿåˆ—å¤§å°
        queue_key = f"{spider_name}:requests"
        stats['queue_size'] = self.redis_client.llen(queue_key)

        # å»é‡é›†åˆå¤§å°
        dupefilter_key = f"{spider_name}:dupefilter"
        stats['processed_urls'] = self.redis_client.scard(dupefilter_key)

        # ç¼“å­˜æ•°é‡
        cache_pattern = f"{spider_name}:cache:*"
        stats['cached_items'] = len(self.redis_client.keys(cache_pattern))

        return stats

    def monitor(self, spider_name, interval=5):
        """
        æŒç»­ç›‘æ§

        Args:
            spider_name: çˆ¬è™«åç§°
            interval: åˆ·æ–°é—´éš”ï¼ˆç§’ï¼‰
        """
        print(f"ğŸ” Monitoring {spider_name}... (Press Ctrl+C to stop)\n")

        try:
            while True:
                stats = self.get_spider_stats(spider_name)

                print(f"\rğŸ“Š Queue: {stats['queue_size']:>6} | "
                      f"Processed: {stats['processed_urls']:>8} | "
                      f"Cached: {stats['cached_items']:>6}",
                      end='', flush=True)

                time.sleep(interval)

        except KeyboardInterrupt:
            print("\n\nâœ… Monitoring stopped")


if __name__ == '__main__':
    monitor = CrawlerMonitor(
        redis_host='localhost',
        redis_port=6379,
        redis_password='redis123'
    )

    monitor.monitor('target_spider', interval=2)
```

### 10. docker/docker-compose.yml

```yaml
version: "3.8"

services:
  # Redis
  redis:
    image: redis:7-alpine
    container_name: crawler_redis
    command: redis-server --requirepass redis123 --maxmemory 512mb
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - crawler_network

  # MongoDB
  mongodb:
    image: mongo:7.0
    container_name: crawler_mongodb
    environment:
      MONGO_INITDB_ROOT_USERNAME: admin
      MONGO_INITDB_ROOT_PASSWORD: admin123
    ports:
      - "27017:27017"
    volumes:
      - mongodb_data:/data/db
    networks:
      - crawler_network

  # Scrapy Master (è°ƒåº¦å™¨)
  master:
    build:
      context: ..
      dockerfile: docker/Dockerfile.scheduler
    container_name: crawler_master
    environment:
      REDIS_HOST: redis
      REDIS_PORT: 6379
      REDIS_PASSWORD: redis123
      MONGODB_URI: mongodb://admin:admin123@mongodb:27017/
    depends_on:
      - redis
      - mongodb
    networks:
      - crawler_network
    command: python scheduler/task_manager.py

  # Scrapy Worker (çˆ¬è™«èŠ‚ç‚¹)
  spider:
    build:
      context: ..
      dockerfile: docker/Dockerfile.spider
    environment:
      REDIS_HOST: redis
      REDIS_PORT: 6379
      REDIS_PASSWORD: redis123
      MONGODB_URI: mongodb://admin:admin123@mongodb:27017/
    depends_on:
      - redis
      - mongodb
    networks:
      - crawler_network
    deploy:
      replicas: 3
    command: scrapy crawl target_spider

networks:
  crawler_network:
    driver: bridge

volumes:
  redis_data:
  mongodb_data:
```

### 11. requirements.txt

```txt
Scrapy>=2.11.0
scrapy-redis>=0.7.3
redis>=5.0.0
pymongo>=4.6.0
python-dotenv>=1.0.0
w3lib>=2.1.0
itemadapter>=0.8.0
```

---

## ğŸš€ ä½¿ç”¨æŒ‡å—

### å•æœºæµ‹è¯•

```bash
# 1. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 2. å¯åŠ¨ Redis å’Œ MongoDB
docker-compose up -d redis mongodb

# 3. æ·»åŠ ä»»åŠ¡
python scripts/add_tasks.py

# 4. å¯åŠ¨çˆ¬è™«
scrapy crawl target_spider

# 5. ç›‘æ§è¿›åº¦
python scripts/monitor.py
```

### åˆ†å¸ƒå¼éƒ¨ç½²

```bash
# 1. æ„å»ºå¹¶å¯åŠ¨æ‰€æœ‰æœåŠ¡
docker-compose up -d

# 2. æ‰©å®¹çˆ¬è™«èŠ‚ç‚¹
docker-compose up -d --scale spider=5

# 3. æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f spider

# 4. æŸ¥çœ‹ç»Ÿè®¡
docker exec crawler_master python scripts/monitor.py
```

---

## ğŸ“š ç›¸å…³èµ„æº

- [åŸºç¡€çˆ¬è™«é¡¹ç›®](./basic_scraper.md)
- [Docker éƒ¨ç½²](./docker_setup.md)
- [åˆ†å¸ƒå¼çˆ¬è™«](../06-Engineering/distributed_scraping.md)
- [Scrapy æ–‡æ¡£](https://docs.scrapy.org/)
- [scrapy-redis æ–‡æ¡£](https://github.com/rmax/scrapy-redis)
