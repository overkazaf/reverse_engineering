# Docker éƒ¨ç½²é…ç½®æ¨¡æ¿

å®¹å™¨åŒ–éƒ¨ç½²çˆ¬è™«é¡¹ç›®çš„å®Œæ•´é…ç½®ï¼Œæ”¯æŒå•æœºå’Œåˆ†å¸ƒå¼éƒ¨ç½²ã€‚

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
docker_scraper/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ config.py
â”‚   â””â”€â”€ scraper.py
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ Dockerfile.dev
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ entrypoint.sh
â”‚   â””â”€â”€ wait-for-it.sh
â”œâ”€â”€ nginx/
â”‚   â””â”€â”€ nginx.conf
â”œâ”€â”€ .env.example
â”œâ”€â”€ .dockerignore
â””â”€â”€ README.md
```

---

## ğŸ“„ é…ç½®æ–‡ä»¶

### 1. Dockerfile (ç”Ÿäº§ç¯å¢ƒ)

```dockerfile
# åŸºç¡€é•œåƒ
FROM python:3.11-slim

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PIP_NO_CACHE_DIR=1 \
    PIP_DISABLE_PIP_VERSION_CHECK=1

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    curl \
    wget \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .

# å®‰è£… Python ä¾èµ–
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶é¡¹ç›®æ–‡ä»¶
COPY . .

# åˆ›å»ºé root ç”¨æˆ·
RUN useradd -m -u 1000 scraper && \
    chown -R scraper:scraper /app

# åˆ‡æ¢ç”¨æˆ·
USER scraper

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python -c "import sys; sys.exit(0)"

# å¯åŠ¨å‘½ä»¤
CMD ["python", "app/main.py"]
```

### 2. Dockerfile.dev (å¼€å‘ç¯å¢ƒ)

```dockerfile
FROM python:3.11-slim

ENV PYTHONUNBUFFERED=1

WORKDIR /app

# å®‰è£…å¼€å‘å·¥å…·
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    curl \
    wget \
    vim \
    git \
    && rm -rf /var/lib/apt/lists/*

COPY requirements.txt requirements-dev.txt ./

RUN pip install --no-cache-dir -r requirements.txt && \
    pip install --no-cache-dir -r requirements-dev.txt

# ä¸å¤åˆ¶ä»£ç ï¼Œä½¿ç”¨ volume æŒ‚è½½
VOLUME /app

CMD ["python", "app/main.py"]
```

### 3. docker-compose.yml

```yaml
version: "3.8"

services:
  # çˆ¬è™«åº”ç”¨
  scraper:
    build:
      context: .
      dockerfile: docker/Dockerfile
    container_name: scraper_app
    restart: unless-stopped
    environment:
      - MONGODB_URI=mongodb://mongodb:27017/
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - LOG_LEVEL=INFO
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    depends_on:
      mongodb:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - scraper_network
    labels:
      - "com.scraper.description=Main scraper service"

  # MongoDB æ•°æ®åº“
  mongodb:
    image: mongo:7.0
    container_name: scraper_mongodb
    restart: unless-stopped
    environment:
      MONGO_INITDB_ROOT_USERNAME: admin
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_PASSWORD:-admin123}
      MONGO_INITDB_DATABASE: scraper_db
    volumes:
      - mongodb_data:/data/db
      - mongodb_config:/data/configdb
    ports:
      - "27017:27017"
    healthcheck:
      test: echo 'db.runCommand("ping").ok' | mongosh localhost:27017/test --quiet
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
    networks:
      - scraper_network

  # Redis ç¼“å­˜
  redis:
    image: redis:7-alpine
    container_name: scraper_redis
    restart: unless-stopped
    command: redis-server --requirepass ${REDIS_PASSWORD:-redis123} --maxmemory 256mb --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - scraper_network

  # Nginx (å¯é€‰)
  nginx:
    image: nginx:alpine
    container_name: scraper_nginx
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./data/static:/usr/share/nginx/html:ro
    depends_on:
      - scraper
    networks:
      - scraper_network

  # ç›‘æ§ (å¯é€‰)
  prometheus:
    image: prom/prometheus:latest
    container_name: scraper_prometheus
    restart: unless-stopped
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"
    networks:
      - scraper_network

  # å¯è§†åŒ–ç›‘æ§
  grafana:
    image: grafana/grafana:latest
    container_name: scraper_grafana
    restart: unless-stopped
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin123}
    volumes:
      - grafana_data:/var/lib/grafana
    ports:
      - "3000:3000"
    depends_on:
      - prometheus
    networks:
      - scraper_network

networks:
  scraper_network:
    driver: bridge

volumes:
  mongodb_data:
  mongodb_config:
  redis_data:
  prometheus_data:
  grafana_data:
```

### 4. docker-compose.dev.yml (å¼€å‘ç¯å¢ƒ)

```yaml
version: "3.8"

services:
  scraper:
    build:
      context: .
      dockerfile: docker/Dockerfile.dev
    container_name: scraper_dev
    environment:
      - MONGODB_URI=mongodb://mongodb:27017/
      - REDIS_HOST=redis
      - LOG_LEVEL=DEBUG
    volumes:
      - .:/app # æŒ‚è½½ä»£ç ç›®å½•ï¼Œæ”¯æŒçƒ­é‡è½½
      - /app/__pycache__ # æ’é™¤ç¼“å­˜
    ports:
      - "8000:8000"
      - "5678:5678" # è°ƒè¯•ç«¯å£
    depends_on:
      - mongodb
      - redis
    networks:
      - scraper_network
    command: python -m debugpy --listen 0.0.0.0:5678 --wait-for-client app/main.py

  mongodb:
    image: mongo:7.0
    container_name: scraper_mongodb_dev
    environment:
      MONGO_INITDB_ROOT_USERNAME: admin
      MONGO_INITDB_ROOT_PASSWORD: admin123
    ports:
      - "27017:27017"
    volumes:
      - mongodb_dev_data:/data/db
    networks:
      - scraper_network

  redis:
    image: redis:7-alpine
    container_name: scraper_redis_dev
    command: redis-server --requirepass redis123
    ports:
      - "6379:6379"
    networks:
      - scraper_network

networks:
  scraper_network:
    driver: bridge

volumes:
  mongodb_dev_data:
```

### 5. .dockerignore

```
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
venv/
env/
ENV/
*.egg-info/
dist/
build/

# æ•°æ®å’Œæ—¥å¿—
data/
logs/
*.log

# Git
.git/
.gitignore

# IDE
.vscode/
.idea/
*.swp
*.swo

# ç¯å¢ƒå˜é‡
.env
.env.local

# Docker
docker-compose.yml
Dockerfile*

# æµ‹è¯•
.pytest_cache/
.coverage
htmlcov/

# æ–‡æ¡£
docs/
*.md
!README.md
```

### 6. scripts/entrypoint.sh

```bash
#!/bin/bash
set -e

echo "ğŸš€ Starting scraper application..."

# ç­‰å¾… MongoDB å°±ç»ª
echo "â³ Waiting for MongoDB..."
until python -c "
import pymongo
import sys
try:
    client = pymongo.MongoClient('${MONGODB_URI}', serverSelectionTimeoutMS=2000)
    client.server_info()
    print('âœ… MongoDB is ready!')
except Exception as e:
    print(f'âŒ MongoDB not ready: {e}')
    sys.exit(1)
"; do
  echo "MongoDB is unavailable - sleeping"
  sleep 2
done

# ç­‰å¾… Redis å°±ç»ª
echo "â³ Waiting for Redis..."
until python -c "
import redis
import sys
try:
    r = redis.Redis(host='${REDIS_HOST}', port=${REDIS_PORT}, password='${REDIS_PASSWORD}', socket_connect_timeout=2)
    r.ping()
    print('âœ… Redis is ready!')
except Exception as e:
    print(f'âŒ Redis not ready: {e}')
    sys.exit(1)
"; do
  echo "Redis is unavailable - sleeping"
  sleep 2
done

# åˆå§‹åŒ–æ•°æ®åº“ (å¯é€‰)
echo "ğŸ”§ Initializing database..."
python -c "
from app.database import init_db
init_db()
print('âœ… Database initialized!')
"

echo "âœ¨ All services are ready!"
echo "ğŸ¯ Starting main application..."

# æ‰§è¡Œä¸»å‘½ä»¤
exec "$@"
```

### 7. scripts/wait-for-it.sh

```bash
#!/bin/bash
# wait-for-it.sh - ç­‰å¾…æœåŠ¡å¯ç”¨

TIMEOUT=15
QUIET=0
HOST=""
PORT=""

while [ $# -gt 0 ]; do
  case "$1" in
    *:* )
      HOST=$(printf "%s\n" "$1"| cut -d : -f 1)
      PORT=$(printf "%s\n" "$1"| cut -d : -f 2)
      shift 1
      ;;
    -t)
      TIMEOUT="$2"
      shift 2
      ;;
    -q)
      QUIET=1
      shift 1
      ;;
    *)
      echo "Unknown argument: $1"
      exit 1
      ;;
  esac
done

if [ "$HOST" = "" ] || [ "$PORT" = "" ]; then
  echo "Error: you need to provide a host and port to test."
  exit 1
fi

TIMEOUT_END=$(($(date +%s) + TIMEOUT))

while :; do
  if [ $QUIET -ne 1 ]; then
    echo "Waiting for $HOST:$PORT..."
  fi

  nc -z "$HOST" "$PORT" > /dev/null 2>&1
  result=$?

  if [ $result -eq 0 ]; then
    if [ $QUIET -ne 1 ]; then
      echo "$HOST:$PORT is available!"
    fi
    exit 0
  fi

  if [ "$(date +%s)" -ge "$TIMEOUT_END" ]; then
    echo "Timeout waiting for $HOST:$PORT"
    exit 1
  fi

  sleep 1
done
```

### 8. nginx/nginx.conf

```nginx
user nginx;
worker_processes auto;
error_log /var/log/nginx/error.log warn;
pid /var/run/nginx.pid;

events {
    worker_connections 1024;
    use epoll;
}

http {
    include /etc/nginx/mime.types;
    default_type application/octet-stream;

    # æ—¥å¿—æ ¼å¼
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for"';

    access_log /var/log/nginx/access.log main;

    # æ€§èƒ½ä¼˜åŒ–
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    types_hash_max_size 2048;

    # Gzip å‹ç¼©
    gzip on;
    gzip_vary on;
    gzip_min_length 1000;
    gzip_types text/plain text/css application/json application/javascript text/xml application/xml;

    # ä¸Šæ¸¸æœåŠ¡å™¨
    upstream scraper_backend {
        server scraper:8000;
        keepalive 32;
    }

    # ä¸»æœåŠ¡å™¨é…ç½®
    server {
        listen 80;
        server_name localhost;

        # é™æ€æ–‡ä»¶
        location /static/ {
            alias /usr/share/nginx/html/;
            expires 30d;
            add_header Cache-Control "public, immutable";
        }

        # API ä»£ç†
        location /api/ {
            proxy_pass http://scraper_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;

            # è¶…æ—¶è®¾ç½®
            proxy_connect_timeout 60s;
            proxy_send_timeout 60s;
            proxy_read_timeout 60s;
        }

        # å¥åº·æ£€æŸ¥
        location /health {
            access_log off;
            return 200 "healthy\n";
            add_header Content-Type text/plain;
        }
    }
}
```

### 9. .env.example

```bash
# MongoDB é…ç½®
MONGODB_URI=mongodb://mongodb:27017/
MONGO_PASSWORD=admin123

# Redis é…ç½®
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_PASSWORD=redis123

# åº”ç”¨é…ç½®
LOG_LEVEL=INFO
DEBUG=false

# Grafana é…ç½®
GRAFANA_PASSWORD=admin123

# ä»£ç†é…ç½® (å¯é€‰)
USE_PROXY=false
PROXY_URL=http://proxy:7890
```

### 10. monitoring/prometheus.yml

```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  # Prometheus è‡ªèº«
  - job_name: "prometheus"
    static_configs:
      - targets: ["localhost:9090"]

  # çˆ¬è™«åº”ç”¨
  - job_name: "scraper"
    static_configs:
      - targets: ["scraper:8000"]
    metrics_path: "/metrics"

  # MongoDB Exporter (å¦‚æœä½¿ç”¨)
  - job_name: "mongodb"
    static_configs:
      - targets: ["mongodb-exporter:9216"]

  # Redis Exporter (å¦‚æœä½¿ç”¨)
  - job_name: "redis"
    static_configs:
      - targets: ["redis-exporter:9121"]
```

### 11. requirements-dev.txt

```txt
# å¼€å‘å·¥å…·
debugpy>=1.8.0
ipython>=8.18.0
pytest>=7.4.0
pytest-cov>=4.1.0
pytest-mock>=3.12.0
black>=23.12.0
flake8>=6.1.0
mypy>=1.7.0
```

---

## ğŸš€ ä½¿ç”¨æŒ‡å—

### ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

```bash
# 1. å…‹éš†é¡¹ç›®
git clone <your-repo>
cd docker_scraper

# 2. é…ç½®ç¯å¢ƒå˜é‡
cp .env.example .env
vim .env  # ä¿®æ”¹å¯†ç ç­‰é…ç½®

# 3. æ„å»ºå¹¶å¯åŠ¨
docker-compose up -d

# 4. æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f scraper

# 5. æŸ¥çœ‹çŠ¶æ€
docker-compose ps

# 6. åœæ­¢æœåŠ¡
docker-compose down
```

### å¼€å‘ç¯å¢ƒ

```bash
# ä½¿ç”¨å¼€å‘é…ç½®
docker-compose -f docker-compose.dev.yml up

# è¿›å…¥å®¹å™¨
docker exec -it scraper_dev bash

# è¿è¡Œæµ‹è¯•
docker exec scraper_dev pytest

# æŸ¥çœ‹æ—¥å¿—
docker-compose -f docker-compose.dev.yml logs -f
```

### å¸¸ç”¨å‘½ä»¤

```bash
# é‡æ–°æ„å»ºé•œåƒ
docker-compose build --no-cache

# åªå¯åŠ¨ç‰¹å®šæœåŠ¡
docker-compose up -d mongodb redis

# æŸ¥çœ‹èµ„æºä½¿ç”¨
docker stats

# æ¸…ç†æœªä½¿ç”¨çš„èµ„æº
docker system prune -a

# å¯¼å‡ºæ•°æ®
docker exec scraper_mongodb mongodump --out /tmp/backup
docker cp scraper_mongodb:/tmp/backup ./backup

# å¯¼å…¥æ•°æ®
docker cp ./backup scraper_mongodb:/tmp/
docker exec scraper_mongodb mongorestore /tmp/backup
```

---

## ğŸ“Š ç›‘æ§å’Œç»´æŠ¤

### è®¿é—® Grafana

```
URL: http://localhost:3000
ç”¨æˆ·å: admin
å¯†ç : (åœ¨ .env ä¸­è®¾ç½®çš„ GRAFANA_PASSWORD)
```

### æŸ¥çœ‹ Prometheus æŒ‡æ ‡

```
URL: http://localhost:9090
```

### å¥åº·æ£€æŸ¥

```bash
# åº”ç”¨å¥åº·æ£€æŸ¥
curl http://localhost/health

# Docker å¥åº·çŠ¶æ€
docker inspect --format='{{.State.Health.Status}}' scraper_app
```

---

## ğŸ”§ è¿›é˜¶é…ç½®

### å¤šå‰¯æœ¬éƒ¨ç½²

```yaml
# docker-compose.yml
services:
  scraper:
    # ... å…¶ä»–é…ç½®
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: "0.5"
          memory: 512M
        reservations:
          cpus: "0.25"
          memory: 256M
```

### ä½¿ç”¨ Docker Swarm

```bash
# åˆå§‹åŒ– Swarm
docker swarm init

# éƒ¨ç½²æœåŠ¡æ ˆ
docker stack deploy -c docker-compose.yml scraper_stack

# æŸ¥çœ‹æœåŠ¡
docker service ls

# æ‰©å®¹
docker service scale scraper_stack_scraper=5
```

### ä½¿ç”¨å¤–éƒ¨é…ç½®

```yaml
# docker-compose.yml
services:
  scraper:
    configs:
      - source: app_config
        target: /app/config.yaml

configs:
  app_config:
    file: ./config.yaml
```

---

## ğŸ“š ç›¸å…³èµ„æº

- [Docker éƒ¨ç½²](../06-Engineering/docker_deployment.md)
- [åˆ†å¸ƒå¼çˆ¬è™«](./distributed_crawler.md)
- [ç›‘æ§å‘Šè­¦](../06-Engineering/monitoring_and_alerting.md)
