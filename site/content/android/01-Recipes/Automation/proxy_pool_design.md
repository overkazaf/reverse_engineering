---
title: "ä»£ç†æ± è®¾è®¡ä¸ Scrapy é›†æˆ"
date: 2024-08-17
type: posts
tags: ["Docker", "ä»£ç†æ± ", "ç”µå•†", "è‡ªåŠ¨åŒ–", "Android", "ç¤¾äº¤åª’ä½“"]
weight: 10
---

# ä»£ç†æ± è®¾è®¡ä¸ Scrapy é›†æˆ

> **ğŸ“š å‰ç½®çŸ¥è¯†**
>
> æœ¬é…æ–¹æ¶‰åŠä»¥ä¸‹æ ¸å¿ƒæŠ€æœ¯ï¼Œå»ºè®®å…ˆé˜…è¯»ç›¸å…³ç« èŠ‚ï¼š
>
> - **[Scrapy å¿«é€Ÿå…¥é—¨](./scrapy.md)** - ç†è§£ Scrapy ä¸­é—´ä»¶æœºåˆ¶
> - **Redis åŸºç¡€** - ç†è§£ Redis æ•°æ®ç»“æ„ä¸æ“ä½œ

åœ¨é¢å¯¹åçˆ¬è™«ç­–ç•¥ä¸¥å‰çš„ç›®æ ‡ç«™ï¼ˆå¦‚ç”µå•†ã€ç¤¾äº¤åª’ä½“ï¼‰æ—¶ï¼Œå•ä¸€ IP å¾ˆå®¹æ˜“è¢«å°ç¦ã€‚æ„å»ºä¸€ä¸ªé«˜å¯ç”¨ã€è‡ªåŠ¨è½®è½¬çš„ä»£ç†æ±  (Proxy Pool) æ˜¯å¤§è§„æ¨¡æ•°æ®é‡‡é›†çš„åŸºç¡€è®¾æ–½ã€‚

---

## 1. ä»£ç†æ± æ¶æ„è®¾è®¡

ä¸€ä¸ªæˆç†Ÿçš„ä»£ç†æ± ç³»ç»Ÿé€šå¸¸åŒ…å«å››ä¸ªç‹¬ç«‹æ¨¡å—ï¼Œé€šè¿‡ Redis è¿›è¡Œè§£è€¦ï¼š

### æ ¸å¿ƒç»„ä»¶

1. **Fetcher (è·å–å™¨)**:
   - **èŒè´£**: å®šæ—¶ä»å„å¤§å…è´¹ä»£ç†ç½‘ç«™ï¼ˆå¿«ä»£ç†ã€äº‘ä»£ç†ç­‰ï¼‰æˆ–ä»˜è´¹ API æ¥å£æ‹‰å–ä»£ç† IPã€‚
   - **ç­–ç•¥**: æ¯éš” N åˆ†é’Ÿè¿è¡Œä¸€æ¬¡ï¼Œå°†è·å–åˆ°çš„æ–° IP å­˜å…¥ Redis çš„"å¾…æ£€æµ‹"é˜Ÿåˆ—ã€‚

2. **Checker (æ£€æµ‹å™¨)**:
   - **èŒè´£**: å¼‚æ­¥æ£€æµ‹ Redis ä¸­ä»£ç† IP çš„å¯ç”¨æ€§ã€åŒ¿ååº¦å’Œå“åº”é€Ÿåº¦ã€‚
   - **å®ç°**: ä½¿ç”¨ `aiohttp` æˆ– `requests` å¯¹ç›®æ ‡ç½‘ç«™ï¼ˆå¦‚ç™¾åº¦ã€è°·æ­Œæˆ–ç‰¹å®šç›®æ ‡ç«™ï¼‰å‘èµ·è¯·æ±‚ã€‚
   - **è¯„åˆ†æœºåˆ¶**:

| é¡¹ç›®       | è¯´æ˜                                                        |
| ---------- | ----------------------------------------------------------- |
| **å¯ç”¨**   | åˆ†æ•°è®¾ä¸º 100ï¼ˆæˆ– +1ï¼‰ã€‚                                     |
| **ä¸å¯ç”¨** | åˆ†æ•°å‡ 1ï¼Œå½“åˆ†æ•°ä½äºé˜ˆå€¼ï¼ˆå¦‚ 0ï¼‰æ—¶ï¼Œä» Redis ç§»é™¤ã€‚         |
| **å¤æ£€**   | å®šæ—¶éå† Redis ä¸­ç°å­˜çš„ä»£ç†è¿›è¡Œå¤æ£€ï¼Œç¡®ä¿åº“ä¸­ IP å§‹ç»ˆæœ‰æ•ˆã€‚ |

3. **Storage (å­˜å‚¨å™¨)**:

| é¡¹ç›®         | è¯´æ˜                                                  |
| ------------ | ----------------------------------------------------- |
| **æ•°æ®åº“**   | Redis æ˜¯æœ€ä½³é€‰æ‹©ã€‚                                    |
| **æ•°æ®ç»“æ„** | `Sorted Set` (æœ‰åºé›†åˆ)ã€‚                             |
| **Key**      | ä»£ç† IP (`1.2.3.4:8080`)                              |
| **Score**    | ä»£ç†åˆ†æ•° (0-100)                                      |
| **ä¼˜åŠ¿**     | å¯ä»¥åˆ©ç”¨ `ZRANGEBYSCORE` è½»æ¾è·å–é«˜è´¨é‡ï¼ˆæ»¡åˆ†ï¼‰ä»£ç†ã€‚ |

4. **API Server (æ¥å£æœåŠ¡)**:
   - **èŒè´£**: ä¸ºçˆ¬è™«æä¾›ç®€å•çš„ HTTP æ¥å£è·å–ä»£ç†ã€‚
   - **æ¥å£**:
     - `/get`: éšæœºè¿”å›ä¸€ä¸ªé«˜åˆ†ä»£ç†ã€‚
     - `/count`: æŸ¥çœ‹å½“å‰å¯ç”¨ä»£ç†æ•°é‡ã€‚

### æ¶æ„å›¾

```mermaid
graph LR
    ProxySources[å…è´¹/ä»˜è´¹æº] --> Fetcher
    Fetcher -->|Raw Proxy| Redis[(Redis Sorted Set)]
    Redis <-->|Validation| Checker
    Crawler[Scrapy çˆ¬è™«] -->|Request| API[API Server]
    API -->|Get High Score Proxy| Redis
```

---

## 2. Scrapy ä¸­é—´ä»¶é›†æˆ

### å·¥ä½œæµç¨‹

1. **è¯·æ±‚å‰ (`process_request`)**: ä»ä»£ç†æ± è·å–ä¸€ä¸ªä»£ç†ï¼Œèµ‹å€¼ç»™ `request.meta['proxy']`ã€‚
2. **å“åº”å (`process_response`)**: æ£€æŸ¥çŠ¶æ€ç ã€‚å¦‚æœæ˜¯ 200ï¼Œè¯´æ˜ä»£ç†æ­£å¸¸ï¼›å¦‚æœæ˜¯ 403/429/è¶…æ—¶ï¼Œè¯´æ˜ä»£ç†å¯èƒ½å¤±æ•ˆæˆ–è¢«å°ã€‚
3. **å¼‚å¸¸å¤„ç† (`process_exception`)**: æ•è·è¿æ¥è¶…æ—¶ã€è¿æ¥æ‹’ç»ç­‰ç½‘ç»œé”™è¯¯ï¼Œæ ‡è®°è¯¥ä»£ç†å¤±æ•ˆï¼Œå¹¶å¯¹å½“å‰è¯·æ±‚è¿›è¡Œé‡è¯•ã€‚

### ä»£ç å®ç°

```python
# middlewares.py
import requests
import logging
from scrapy.exceptions import IgnoreRequest

class ProxyMiddleware:
    def __init__(self, proxy_pool_url):
        self.proxy_pool_url = proxy_pool_url
        self.logger = logging.getLogger(__name__)

    @classmethod
    def from_crawler(cls, crawler):
        return cls(
            proxy_pool_url=crawler.settings.get('PROXY_POOL_URL')
        )

    def _get_random_proxy(self):
        try:
            response = requests.get(self.proxy_pool_url)
            if response.status_code == 200:
                return response.text.strip()
        except requests.ConnectionError:
            return None
        return None

    def process_request(self, request, spider):
        # å¦‚æœè¯·æ±‚å·²ç»è®¾ç½®ä»£ç†ï¼ˆä¾‹å¦‚ç‰¹å®šè¯·æ±‚ï¼‰ï¼Œåˆ™è·³è¿‡
        if request.meta.get('proxy'):
            return

        proxy = self._get_random_proxy()
        if proxy:
            self.logger.debug(f"Using proxy: {proxy}")
            # è®¾ç½®ä»£ç†ï¼Œæ ¼å¼: http://user:pass@ip:ç«¯å£ æˆ– http://ip:ç«¯å£
            request.meta['proxy'] = f"http://{proxy}"
        else:
            self.logger.warning("No proxy available from pool!")

    def process_response(self, request, response, spider):
        # å¦‚æœé‡åˆ°éªŒè¯ç ã€å°ç¦ç­‰çŠ¶æ€ç 
        if response.status in [403, 429]:
            self.logger.warning(
                f"Proxy {request.meta.get('proxy')} banned "
                f"(Status {response.status}), retrying..."
            )
            # æ ‡è®°è¯¥ä»£ç†å¤±æ•ˆï¼ˆå¯é€‰ï¼šè°ƒç”¨æ¥å£æŠ¥å‘Šè¯¥ä»£ç†åï¼‰
            # self._report_bad_proxy(request.meta.get('proxy'))

            # åˆ é™¤å½“å‰ä»£ç†è®¾ç½®ï¼Œé‡æ–°è°ƒåº¦è¯·æ±‚ï¼ˆä¼šå†æ¬¡ç»è¿‡ process_request æ¢æ–°ä»£ç†ï¼‰
            del request.meta['proxy']
            return request.replace(dont_filter=True)

        return response

    def process_exception(self, request, exception, spider):
        # å¤„ç†è¿æ¥è¶…æ—¶ã€DNS é”™è¯¯ç­‰
        self.logger.error(f"Proxy {request.meta.get('proxy')} failed: {exception}")

        # æ¢ä»£ç†é‡è¯•
        if 'proxy' in request.meta:
            del request.meta['proxy']
        return request.replace(dont_filter=True)
```

### é…ç½® settings.py

```python
# settings.py
DOWNLOADER_MIDDLEWARES = {
    'myproject.middlewares.ProxyMiddleware': 543,
    # ç¦ç”¨ Scrapy é»˜è®¤ UserAgent å’Œé‡è¯•ä¸­é—´ä»¶ï¼ˆè§†æƒ…å†µè€Œå®šï¼‰
    # 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,
}

PROXY_POOL_URL = 'http://localhost:5000/get'
```

---

## 3. å¼€æºä»£ç†æ± æ¨è

1. **proxy_pool**
   - **GitHub**: `jhao104/proxy_pool`
   - **ç‰¹ç‚¹**: ä¹Ÿæ˜¯åŸºäº Redisï¼Œæ¶æ„æ¸…æ™°ï¼Œæ”¯æŒ Docker ä¸€é”®éƒ¨ç½²ï¼Œå†…ç½®äº†å‡ åä¸ªå…è´¹æºçš„æŠ“å–è§„åˆ™ã€‚

2. **Gerapy / Scylla**
   - **GitHub**: `imWildCat/scylla`
   - **ç‰¹ç‚¹**: æ™ºèƒ½åŒ–ä»£ç†æ± ï¼Œè‡ªåŠ¨å­¦ä¹ ä»£ç†çš„ç¨³å®šæ€§ã€‚

3. **GimmeProxy**
   - **ç‰¹ç‚¹**: Go è¯­è¨€ç¼–å†™ï¼Œæ€§èƒ½å¼ºåŠ²ã€‚

---

## 4. éš§é“ä»£ç† (Tunnel Proxy)

å¯¹äºä¼ä¸šçº§åº”ç”¨ï¼Œç»´æŠ¤è‡ªå»ºä»£ç†æ± æˆæœ¬è¾ƒé«˜ï¼ˆå…è´¹ IP è´¨é‡æå·®ï¼Œå¯ç”¨ç‡ä¸è¶³ 5%ï¼‰ã€‚æ­¤æ—¶é€šå¸¸ä½¿ç”¨å‚å•†æä¾›çš„**éš§é“ä»£ç†**ã€‚

**ç‰¹ç‚¹**:

- ä¸éœ€è¦åœ¨æœ¬åœ°ç»´æŠ¤ IP æ± ã€‚
- åªæœ‰ä¸€ä¸ªå›ºå®šçš„å…¥å£åœ°å€ï¼ˆå¦‚ `http://proxy.vendor.com:8000`ï¼‰ã€‚
- **æ¯ä¸€æ¬¡è¯·æ±‚ï¼Œäº‘ç«¯ä¼šè‡ªåŠ¨è½¬å‘ç»™èƒŒåä¸åŒçš„åŠ¨æ€ IP**ã€‚

**Scrapy é›†æˆ**:

åªéœ€è¦åœ¨ `process_request` ä¸­å°†ä»£ç†è®¾ç½®ä¸ºè¯¥å›ºå®šåœ°å€ï¼Œå¹¶åœ¨ Header ä¸­æ·»åŠ é‰´æƒä¿¡æ¯ã€‚

```python
# Tunnel Proxy Example
import base64

def process_request(self, request, spider):
    request.meta['proxy'] = "http://proxy.vendor.com:8000"
    # æŸäº›å‚å•†è¦æ±‚åœ¨å¤´éƒ¨é€šè¿‡ Proxy-Authorization è®¤è¯
    auth = base64.b64encode(b"user:pass").decode()
    request.headers['Proxy-Authorization'] = f"Basic {auth}"
```

---

## æ€»ç»“

ä»£ç†æ± æ˜¯å¤§è§„æ¨¡çˆ¬è™«ç³»ç»Ÿçš„æ ¸å¿ƒåŸºç¡€è®¾æ–½ã€‚é€šè¿‡åˆç†çš„æ¶æ„è®¾è®¡ï¼ˆFetcher â†’ Checker â†’ Storage â†’ APIï¼‰å’Œ Scrapy ä¸­é—´ä»¶é›†æˆï¼Œå¯ä»¥æ„å»ºä¸€ä¸ªé«˜å¯ç”¨ã€è‡ªåŠ¨è½®è½¬çš„ä»£ç†ç³»ç»Ÿï¼Œæœ‰æ•ˆåº”å¯¹ç›®æ ‡ç«™çš„åçˆ¬è™«ç­–ç•¥ã€‚
