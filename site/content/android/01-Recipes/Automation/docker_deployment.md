---
title: "å®¹å™¨åŒ–éƒ¨ç½²ï¼šDocker ä¸ Kubernetes å®æˆ˜"
date: 2025-05-23
tags: ["Docker", "è‡ªåŠ¨åŒ–", "Android", "Rootæ£€æµ‹", "Kubernetes"]
weight: 10
---

# å®¹å™¨åŒ–éƒ¨ç½²ï¼šDocker ä¸ Kubernetes å®æˆ˜

> **ğŸ“š å‰ç½®çŸ¥è¯†**
>
> æœ¬é…æ–¹æ¶‰åŠä»¥ä¸‹æ ¸å¿ƒæŠ€æœ¯ï¼Œå»ºè®®å…ˆé˜…è¯»ç›¸å…³ç« èŠ‚ï¼š
>
> - **[Scrapy å¿«é€Ÿå…¥é—¨](./scrapy.md)** - ç†è§£ Scrapy é¡¹ç›®ç»“æ„
> - **Docker åŸºç¡€** - ç†è§£å®¹å™¨ä¸é•œåƒæ¦‚å¿µ

å°†çˆ¬è™«é¡¹ç›®å®¹å™¨åŒ–æ˜¯å®ç°æ ‡å‡†åŒ–éƒ¨ç½²ã€å¼¹æ€§ä¼¸ç¼©å’Œ CI/CD çš„ç¬¬ä¸€æ­¥ã€‚æœ¬æŒ‡å—å°†è¯¦ç»†ä»‹ç»å¦‚ä½•ç¼–å†™ Dockerfileï¼Œä½¿ç”¨ Docker Compose ç¼–æ’æœåŠ¡ï¼Œä»¥åŠå¦‚ä½•åœ¨ Kubernetes (K8s) ä¸Šè¿è¡Œçˆ¬è™«ä»»åŠ¡ã€‚

---

## 1. Dockerfile æœ€ä½³å®è·µ

æˆ‘ä»¬éœ€è¦ä¸º Scrapy é¡¹ç›®æ„å»ºä¸€ä¸ªè½»é‡ã€ç¨³å®šçš„ Docker é•œåƒã€‚

### ç›®å½•ç»“æ„

```text
my_crawler/
â”œâ”€â”€ scrapy.cfg
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â””â”€â”€ myproject/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ items.py
    â”œâ”€â”€ settings.py
    â””â”€â”€ spiders/
```

### Dockerfile

```dockerfile
FROM python:3.9-slim-buster

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# è®¾ç½®ç¯å¢ƒå˜é‡
# é˜²æ­¢ Python ç”Ÿæˆ .pyc æ–‡ä»¶
ENV PYTHONDONTWRITEBYTECODE 1
# é˜²æ­¢ Python ç¼“å†²åŒº stdout/stderrï¼Œç¡®ä¿æ—¥å¿—å®æ—¶è¾“å‡º
ENV PYTHONUNBUFFERED 1
# è®¾ç½®æ—¶åŒº (å¯é€‰)
ENV TZ=Asia/Shanghai

# å®‰è£…ç³»ç»Ÿä¾èµ– (å¦‚æœéœ€è¦ç¼–è¯‘ lxml æˆ–å…¶å®ƒåº“)
# RUN apt-get update && apt-get install -y gcc libxml2-dev libxslt-dev && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶å¹¶å®‰è£…
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶é¡¹ç›®ä»£ç 
COPY . .

# é»˜è®¤å¯åŠ¨å‘½ä»¤ (å¯è¢« docker run è¦†ç›–)
# è¿™é‡Œæˆ‘ä»¬é»˜è®¤å¯åŠ¨ scrapyd (å¦‚æœä½¿ç”¨ scrapyd éƒ¨ç½²) æˆ–ä»…ä½œä¸ºä¸€ä¸ª shell
CMD ["scrapy", "list"]
```

### æ„å»ºä¸è¿è¡Œ

```bash
# æ„å»ºé•œåƒ
docker build -t my-crawler:v1 .

# è¿è¡Œçˆ¬è™«
docker run --rm my-crawler:v1 scrapy crawl myspider
```

---

## 2. Docker Compose ç¼–æ’

å¯¹äºåˆ†å¸ƒå¼ Scrapy-Redis æ¶æ„ï¼Œæˆ‘ä»¬éœ€è¦åŒæ—¶è¿è¡Œ Redisã€MongoDB å’Œå¤šä¸ªçˆ¬è™«èŠ‚ç‚¹ã€‚

### docker-compose.yml

```yaml
version: '3.8'

services:
  # 1. Redis æœåŠ¡ (æ¶ˆæ¯é˜Ÿåˆ—)
  redis:
    image: redis:6.2-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes

  # 2. MongoDB æœåŠ¡ (æ•°æ®å­˜å‚¨)
  mongo:
    image: mongo:5.0
    ports:
      - "27017:27017"
    environment:
      MONGO_INITDB_ROOT_USERNAME: admin
      MONGO_INITDB_ROOT_PASSWORD: password
    volumes:
      - mongo_data:/data/db

  # 3. çˆ¬è™«æœåŠ¡ (Master/Slave æ¨¡å¼ä¸­çš„ Slave)
  crawler:
    build: .
    image: my-crawler:latest
    # è¦†ç›–é»˜è®¤å‘½ä»¤ï¼Œå¯åŠ¨çˆ¬è™«
    command: scrapy crawl myspider_distributed
    # ä¾èµ–æœåŠ¡å°±ç»ª
    depends_on:
      - redis
      - mongo
    environment:
      - REDIS_HOST=redis
      - MONGO_URI=mongodb://admin:${MONGO_PASSWORD}@mongo:27017
    # æƒ³è¦å¼€å¯å¤šä¸ªçˆ¬è™«èŠ‚ç‚¹ï¼Ÿç›´æ¥ scale
    deploy:
      replicas: 3

volumes:
  redis_data:
  mongo_data:
```

### å¯åŠ¨ä¸æ‰©å®¹

```bash
# å¯åŠ¨æ‰€æœ‰æœåŠ¡
docker-compose up -d

# æ‰©å®¹çˆ¬è™«èŠ‚ç‚¹åˆ° 5
docker-compose up -d --scale crawler=5

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f crawler

# åœæ­¢æ‰€æœ‰æœåŠ¡
docker-compose down
```

---

## 3. Kubernetes éƒ¨ç½²

åœ¨ K8s ä¸Šè¿è¡Œçˆ¬è™«ï¼Œå¯ä»¥åˆ©ç”¨å…¶å¼ºå¤§çš„è°ƒåº¦å’Œè‡ªæ„ˆèƒ½åŠ›ã€‚

### Deploymentï¼ˆæŒç»­è¿è¡Œçš„çˆ¬è™« Workerï¼‰

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: scrapy-worker
  labels:
    app: scrapy-worker
spec:
  # å‰¯æœ¬æ•°ï¼Œå³å¹¶å‘çˆ¬è™«èŠ‚ç‚¹æ•°é‡
  replicas: 5
  selector:
    matchLabels:
      app: scrapy-worker
  template:
    metadata:
      labels:
        app: scrapy-worker
    spec:
      containers:
        - name: crawler
          image: registry.example.com/my-crawler:v1
          # å®¹å™¨å¯åŠ¨å‘½ä»¤
          command: ["scrapy", "crawl", "myspider_distributed"]
          # ç¯å¢ƒå˜é‡é…ç½®
          env:
            - name: REDIS_HOST
              value: "redis-service"  # K8s Service Name
            - name: MONGO_URI
              valueFrom:
                secretKeyRef:
                  name: db-secrets
                  key: mongo-uri
          resources:
            requests:
              memory: "256Mi"
              cpu: "250m"
            limits:
              memory: "512Mi"
              cpu: "500m"
```

### CronJobï¼ˆå®šæ—¶ä»»åŠ¡ï¼‰

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: daily-crawler
spec:
  # æ¯å¤©å‡Œæ™¨ 2 ç‚¹è¿è¡Œ
  schedule: "0 2 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - name: crawler
              image: registry.example.com/my-crawler:v1
              command: ["scrapy", "crawl", "daily_spider"]
              env:
                - name: REDIS_HOST
                  value: "redis-service"
          restartPolicy: OnFailure
```

### å¸¸ç”¨ kubectl å‘½ä»¤

```bash
# åº”ç”¨é…ç½®
kubectl apply -f crawler-deployment.yaml

# æŸ¥çœ‹ Pod çŠ¶æ€
kubectl get pods

# åŠ¨æ€æ‰©ç¼©å®¹ (æ— éœ€ä¿®æ”¹ yaml)
kubectl scale deployment scrapy-worker --replicas=10

# æŸ¥çœ‹æ—¥å¿—
kubectl logs -f deployment/scrapy-worker

# è¿›å…¥å®¹å™¨è°ƒè¯•
kubectl exec -it <pod-name> -- /bin/bash
```

---

## 4. é›†æˆ Scrapyd ç®¡ç†å¹³å°

**Scrapyd**: Scrapy å®˜æ–¹çš„éƒ¨ç½²æœåŠ¡ï¼Œæä¾› HTTP API æ¥éƒ¨ç½²ã€å¯åŠ¨ã€åœæ­¢çˆ¬è™«ã€‚

**Gerapy**: åŸºäº Scrapyd çš„åˆ†å¸ƒå¼ç®¡ç† GUIï¼Œæ”¯æŒèŠ‚ç‚¹ç®¡ç†ã€ä»£ç ç¼–è¾‘ã€å®šæ—¶ä»»åŠ¡ã€‚

### Dockerfile (é›†æˆ Scrapyd)

```dockerfile
FROM python:3.9-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install scrapyd scrapy-redis
COPY scrapyd.conf /etc/scrapyd/
EXPOSE 6800
CMD ["scrapyd"]
```

### scrapyd.conf

```ini
[scrapyd]
eggs_dir    = eggs
logs_dir    = logs
items_dir   = items
jobs_to_keep = 5
dbs_dir     = dbs
max_proc    = 0
max_proc_per_cpu = 4
finished_to_keep = 100
poll_interval = 5.0
bind_address = 0.0.0.0
http_port   = 6800
debug       = off
runner      = scrapyd.runner
application = scrapyd.app.application
launcher    = scrapyd.launcher.Launcher
webroot     = scrapyd.website.Root
```

### éƒ¨ç½²çˆ¬è™«åˆ° Scrapyd

```bash
# æ‰“åŒ…é¡¹ç›®
scrapyd-deploy -p myproject

# å¯åŠ¨çˆ¬è™«
curl http://localhost:6800/schedule.json -d project=myproject -d spider=myspider

# æŸ¥çœ‹çˆ¬è™«çŠ¶æ€
curl http://localhost:6800/listjobs.json?project=myproject
```

---

## 5. CI/CD é›†æˆ

### GitHub Actions ç¤ºä¾‹

```yaml
name: Build and Deploy Crawler

on:
  push:
    branches: [main]

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2

      - name: Build Docker image
        run: docker build -t my-crawler:${{ github.sha }} .

      - name: Push to Registry
        run: |
          docker login -u ${{ secrets.DOCKER_USER }} -p ${{ secrets.DOCKER_PASS }}
          docker tag my-crawler:${{ github.sha }} registry.example.com/my-crawler:${{ github.sha }}
          docker push registry.example.com/my-crawler:${{ github.sha }}

      - name: Deploy to K8s
        run: |
          kubectl set image deployment/scrapy-worker crawler=registry.example.com/my-crawler:${{ github.sha }}
```

---

## æ€»ç»“

å®¹å™¨åŒ–éƒ¨ç½²æ˜¯ç°ä»£çˆ¬è™«å·¥ç¨‹çš„æ ‡å‡†å®è·µã€‚é€šè¿‡ Docker å®ç°ç¯å¢ƒä¸€è‡´æ€§ï¼Œé€šè¿‡ Docker Compose ç®€åŒ–æœ¬åœ°å¼€å‘ï¼Œé€šè¿‡ Kubernetes å®ç°ç”Ÿäº§ç¯å¢ƒçš„å¼¹æ€§ä¼¸ç¼©å’Œè‡ªæ„ˆèƒ½åŠ›ã€‚ç»“åˆ Scrapyd å’Œ CI/CD æµç¨‹ï¼Œå¯ä»¥æ„å»ºä¸€ä¸ªå®Œæ•´çš„çˆ¬è™« DevOps ä½“ç³»ã€‚
