---
title: "Scrapy å¿«é€Ÿå…¥é—¨å¤‡å¿˜å½•"
date: 2024-09-20
weight: 10
---

# Scrapy å¿«é€Ÿå…¥é—¨å¤‡å¿˜å½•

> **ğŸ“š å‰ç½®çŸ¥è¯†**
>
> æœ¬é…æ–¹æ¶‰åŠä»¥ä¸‹æ ¸å¿ƒæŠ€æœ¯ï¼Œå»ºè®®å…ˆé˜…è¯»ç›¸å…³ç« èŠ‚ï¼š
>
> - **Python åŸºç¡€** - æŒæ¡ç±»ã€è£…é¥°å™¨ã€ç”Ÿæˆå™¨ç­‰æ¦‚å¿µ
> - **HTTP åè®®** - ç†è§£è¯·æ±‚å“åº”ã€Headersã€Cookies

Scrapy æ˜¯ä¸€ä¸ªç”¨äºç½‘ç»œçˆ¬è™«å’Œæ•°æ®æŠ“å–çš„ã€å¼€æºçš„ã€åä½œå¼çš„ Python æ¡†æ¶ã€‚å®ƒå…·æœ‰é€Ÿåº¦å¿«ã€åŠŸèƒ½å¼ºå¤§ã€å¯æ‰©å±•æ€§é«˜çš„ç‰¹ç‚¹ã€‚æœ¬å¤‡å¿˜å½•ä¸º Scrapy çš„æ ¸å¿ƒæ¦‚å¿µå’Œå¸¸ç”¨å‘½ä»¤æä¾›å¿«é€Ÿå‚è€ƒã€‚

---

## ç›®å½•

- [Scrapy å¿«é€Ÿå…¥é—¨å¤‡å¿˜å½•](#scrapy-å¿«é€Ÿå…¥é—¨å¤‡å¿˜å½•)
  - [ç›®å½•](#ç›®å½•)
    - [æ ¸å¿ƒç»„ä»¶](#æ ¸å¿ƒç»„ä»¶)
    - [é¡¹ç›®å‘½ä»¤](#é¡¹ç›®å‘½ä»¤)
    - [Spider (çˆ¬è™«)](#spider-çˆ¬è™«)
      - [åŸºæœ¬ç»“æ„](#åŸºæœ¬ç»“æ„)
      - [å¤„ç†åˆ†é¡µå’Œé“¾æ¥](#å¤„ç†åˆ†é¡µå’Œé“¾æ¥)

---

### æ ¸å¿ƒç»„ä»¶

Scrapy çš„æ•°æ®æµç”±ä»¥ä¸‹æ ¸å¿ƒç»„ä»¶ååŒå®Œæˆï¼š

1. **Engine (å¼•æ“)**: è´Ÿè´£æ§åˆ¶æ‰€æœ‰ç»„ä»¶ä¹‹é—´çš„æ•°æ®æµï¼Œå¹¶åœ¨ç›¸åº”åŠ¨ä½œå‘ç”Ÿæ—¶è§¦å‘äº‹ä»¶ã€‚
2. **Scheduler (è°ƒåº¦å™¨)**: æ¥æ”¶æ¥è‡ªå¼•æ“çš„è¯·æ±‚ (`Request`)ï¼Œå¹¶å°†å…¶å…¥é˜Ÿï¼Œä»¥ä¾¿åç»­å¼•æ“è¯·æ±‚æ—¶æä¾›ã€‚
3. **Downloader (ä¸‹è½½å™¨)**: è´Ÿè´£è·å–é¡µé¢æ•°æ®ï¼Œå¹¶å°†å…¶æä¾›ç»™å¼•æ“ï¼Œè€Œåç”±å¼•æ“å°†ç»“æœ (`Response`) äº¤ç»™ Spiderã€‚
4. **Spiders (çˆ¬è™«)**: ç”¨æˆ·ç¼–å†™çš„ç”¨äºè§£æ `Response` å¹¶æå– `Item` æˆ–é¢å¤– `Request` çš„ç±»ã€‚
5. **Item Pipeline (é¡¹ç›®ç®¡é“)**: è´Ÿè´£å¤„ç†ç”± Spider æå–å‡ºæ¥çš„ `Item`ã€‚å…¸å‹çš„æ“ä½œåŒ…æ‹¬æ•°æ®æ¸…æ´—ã€éªŒè¯å’ŒæŒä¹…åŒ–ï¼ˆå¦‚å­˜å…¥æ•°æ®åº“ï¼‰ã€‚
6. **Downloader Middlewares (ä¸‹è½½å™¨ä¸­é—´ä»¶)**: ä½äºå¼•æ“å’Œä¸‹è½½å™¨ä¹‹é—´çš„é’©å­ï¼Œç”¨äºåœ¨è¯·æ±‚å‘é€å’Œå“åº”è¿”å›æ—¶è¿›è¡Œè‡ªå®šä¹‰å¤„ç†ï¼ˆå¦‚è®¾ç½® User-Agentã€å¤„ç†ä»£ç†ï¼‰ã€‚
7. **Spider Middlewares (çˆ¬è™«ä¸­é—´ä»¶)**: ä½äºå¼•æ“å’Œ Spider ä¹‹é—´çš„é’©å­ï¼Œç”¨äºå¤„ç† Spider çš„è¾“å…¥ (`Response`) å’Œè¾“å‡º (`Item`, `Request`)ã€‚

![Scrapy Architecture](https://docs.scrapy.org/en/latest/_images/scrapy_architecture.png)

---

### é¡¹ç›®å‘½ä»¤

| å‘½ä»¤                                   | æè¿°                                                                       |
| :------------------------------------- | :------------------------------------------------------------------------- |
| `pip install scrapy`                   | å®‰è£… Scrapy æ¡†æ¶                                                           |
| `scrapy startproject myproject`        | åˆ›å»ºä¸€ä¸ªåä¸º `myproject` çš„æ–°é¡¹ç›®                                          |
| `cd myproject`                         | è¿›å…¥é¡¹ç›®ç›®å½•                                                               |
| `scrapy genspider example example.com` | åœ¨ `spiders` ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ªåä¸º `example` çš„çˆ¬è™«ï¼Œé™å®šåŸŸåä¸º `example.com` |
| `scrapy crawl example`                 | è¿è¡Œåä¸º `example` çš„çˆ¬è™«                                                  |
| `scrapy crawl example -o output.json`  | è¿è¡Œçˆ¬è™«å¹¶å°†æå–çš„æ•°æ®ä¿å­˜ä¸º JSON æ–‡ä»¶                                     |
| `scrapy shell "http://example.com"`    | å¯åŠ¨ä¸€ä¸ªäº¤äº’å¼ Shellï¼Œç”¨äºæµ‹è¯• XPath/CSS é€‰æ‹©å™¨                            |
| `scrapy list`                          | åˆ—å‡ºé¡¹ç›®ä¸­çš„æ‰€æœ‰å¯ç”¨çˆ¬è™«                                                   |

---

### Spider (çˆ¬è™«)

Spider æ˜¯ä½ å®šä¹‰å¦‚ä½•çˆ¬å–æŸä¸ªç½‘ç«™ï¼ˆæˆ–ä¸€ç»„ç½‘ç«™ï¼‰çš„ç±»ï¼ŒåŒ…æ‹¬çˆ¬å–åŠ¨ä½œå’Œå¦‚ä½•ä»é¡µé¢å†…å®¹ä¸­æå–ç»“æ„åŒ–æ•°æ®ã€‚

#### åŸºæœ¬ç»“æ„

```python
# myproject/spiders/example_spider.py
import scrapy

class ExampleSpider(scrapy.Spider):
# çˆ¬è™«çš„å”¯ä¸€æ ‡è¯†åç§°
name = 'example'
# å…è®¸çˆ¬å–çš„åŸŸååˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
allowed_domains = ['example.com']
# çˆ¬è™«å¯åŠ¨æ—¶è¯·æ±‚çš„ URL åˆ—è¡¨
start_urls = ['http://example.com/']

# å¤„ç† start_urls å“åº”çš„é»˜è®¤å›è°ƒæ–¹æ³•
def parse(self, response):
# åœ¨è¿™é‡Œç¼–å†™è§£æé€»è¾‘
pass

```

- `response.css('a::attr(href)').getall()`: æå–æ‰€æœ‰ `<a>` æ ‡ç­¾çš„ `href` å±æ€§ã€‚

- `response.css('div.product > p::text').get()`: æå– `class="product"` çš„ `div` ä¸‹çš„ `p` æ ‡ç­¾æ–‡æœ¬ã€‚

- **XPath è¡¨è¾¾å¼**:
- `response.xpath('//h1/text()').get()`: æå–ç¬¬ä¸€ä¸ª `<h1>` æ ‡ç­¾çš„æ–‡æœ¬ã€‚

- `response.xpath('//a/@href').getall()`: æå–æ‰€æœ‰ `<a>` æ ‡ç­¾çš„ `href` å±æ€§ã€‚

- `response.xpath('//div[@class="product"]/p/text()').get()`: åŒä¸Šã€‚

#### å¤„ç†åˆ†é¡µå’Œé“¾æ¥

åœ¨ `parse` æ–¹æ³•ä¸­ï¼Œä½ å¯ä»¥ `yield` æ–°çš„ `Request` å¯¹è±¡æ¥è·Ÿè¿›é“¾æ¥ã€‚

```python
def parse(self, response):
# ... æå–å½“å‰é¡µé¢æ•°æ® ...

# æå–ä¸‹ä¸€é¡µé“¾æ¥å¹¶ç”Ÿæˆæ–°è¯·æ±‚
next_page = response.css('a.next_page::attr(href)').get()
if next_page is not None:
# response.urljoin() ç”¨äºå¤„ç†ç›¸å¯¹ URL
yield response.follow(next_page, callback=self.parse)

```

```python
# myproject/items.py
import scrapy

class ProductItem(scrapy.Item):
name = scrapy.Field()
price = scrapy.Field()
description = scrapy.Field()

```

item = ProductItem()
item['name'] = response.css('h1.product-name::text').get()
item['price'] = response.css('span.price::text').get()
yield item

````

```python
# myproject/pipelines.py
import sqlite3

class SQLitePipeline:
def open_spider(self, spider):
# çˆ¬è™«å¼€å¯æ—¶è°ƒç”¨
self.connection = sqlite3.connect('products.db')
self.cursor = self.connection.cursor()
self.cursor.execute('CREATE TABLE IF NOT EXISTS products (name TEXT, price TEXT)')

def close_spider(self, spider):
# çˆ¬è™«å…³é—­æ—¶è°ƒç”¨
self.connection.close()

def process_item(self, item, spider):
# æ¯ä¸ª item éƒ½ä¼šè°ƒç”¨
self.cursor.execute('INSERT INTO products (name, price) VALUES (?, ?)', (item['name'], item['price']))
self.connection.commit()
return item # å¿…é¡»è¿”å› item

````

'myproject.pipelines.SQLitePipeline': 300,
}

```

* `DEFAULT_REQUEST_HEADERS`: è®¾ç½®é»˜è®¤çš„è¯·æ±‚å¤´ï¼Œå¦‚ `User-Agent`ã€‚

* `DOWNLOAD_DELAY = 1`: è®¾ç½®ä¸‹è½½å»¶è¿Ÿï¼ˆç§’ï¼‰ï¼Œä»¥é¿å…å¯¹æœåŠ¡å™¨é€ æˆå¤ªå¤§å‹åŠ›ã€‚

* `CONCURRENT_REQUESTS = 16`: å¹¶å‘è¯·æ±‚æ•°ã€‚

* `ITEM_PIPELINES`: æ¿€æ´»å’Œè®¾ç½® Item Pipeline çš„ä¼˜å…ˆçº§ã€‚

* `DOWNLOADER_MIDDLEWARES`: æ¿€æ´»å’Œè®¾ç½®ä¸‹è½½å™¨ä¸­é—´ä»¶çš„ä¼˜å…ˆçº§ã€‚
```
