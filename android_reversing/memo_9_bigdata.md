# ğŸ—ï¸ å¤§æ•°æ®æŠ€æœ¯æ ˆé€Ÿè®°

## ğŸŒŸ å¤§æ•°æ®ç”Ÿæ€æ¦‚è§ˆ

### ğŸ“Š æŠ€æœ¯æ ˆåˆ†å±‚

| å±‚çº§ | æŠ€æœ¯ç»„ä»¶ | ä½œç”¨ | ä»£è¡¨äº§å“ |
|:---|:---|:---|:---|
| **å­˜å‚¨å±‚** | åˆ†å¸ƒå¼å­˜å‚¨ | æµ·é‡æ•°æ®å­˜å‚¨ | HDFSã€HBaseã€Cassandra |
| **è®¡ç®—å±‚** | åˆ†å¸ƒå¼è®¡ç®— | æ•°æ®å¤„ç†ä¸åˆ†æ | Sparkã€Flinkã€MapReduce |
| **æ•°æ®ä»“åº“å±‚** | æ•°æ®ä»“åº“ | ç»“æ„åŒ–æ•°æ®ç®¡ç† | Hiveã€Impalaã€Presto |
| **åè°ƒå±‚** | é›†ç¾¤ç®¡ç† | èµ„æºè°ƒåº¦ä¸åè°ƒ | YARNã€Mesosã€K8s |
| **åº”ç”¨å±‚** | ä¸šåŠ¡åº”ç”¨ | æ•°æ®åˆ†æä¸æŒ–æ˜ | Jupyterã€Zeppelinã€Tableau |

### ğŸ”„ æ•°æ®å¤„ç†æ¨¡å¼

| æ¨¡å¼ | ç‰¹ç‚¹ | å»¶è¿Ÿ | é€‚ç”¨åœºæ™¯ | ä»£è¡¨æŠ€æœ¯ |
|:---|:---|:---|:---|:---|
| **æ‰¹å¤„ç†** | å¤§æ‰¹é‡ã€é«˜åå | åˆ†é’Ÿåˆ°å°æ—¶ | æ•°æ®ä»“åº“ã€æŠ¥è¡¨ | Sparkã€Hiveã€MapReduce |
| **æµå¤„ç†** | å®æ—¶ã€ä½å»¶è¿Ÿ | æ¯«ç§’åˆ°ç§’ | å®æ—¶ç›‘æ§ã€å‘Šè­¦ | Flinkã€Stormã€Kafka Streams |
| **äº¤äº’å¼æŸ¥è¯¢** | å¿«é€Ÿå“åº” | ç§’çº§ | å³å¸­æŸ¥è¯¢ã€æ¢ç´¢ | Impalaã€Prestoã€Druid |

---

## ğŸ—„ï¸ HDFS åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ

### ğŸ—ï¸ æ ¸å¿ƒæ¶æ„
```
Client â†’ NameNode (å…ƒæ•°æ®ç®¡ç†)
           â†“
       DataNode1, DataNode2, DataNode3...
       (æ•°æ®å­˜å‚¨ï¼Œé»˜è®¤3å‰¯æœ¬)
```

### ğŸ”§ å…³é”®ç‰¹æ€§
- **é«˜å®¹é”™æ€§**: æ•°æ®å¤šå‰¯æœ¬å­˜å‚¨
- **é«˜ååé‡**: ä¼˜åŒ–å¤§æ–‡ä»¶è¯»å†™
- **å¯æ‰©å±•æ€§**: æ°´å¹³æ‰©å±•å­˜å‚¨å®¹é‡
- **æ•°æ®æœ¬åœ°æ€§**: è®¡ç®—é è¿‘æ•°æ®

### ğŸ“‹ å¸¸ç”¨æ“ä½œ
```bash
# æ–‡ä»¶æ“ä½œ
hdfs dfs -ls /user/data/
hdfs dfs -put local_file.txt /user/data/
hdfs dfs -get /user/data/file.txt local_file.txt
hdfs dfs -rm /user/data/file.txt

# ç›®å½•æ“ä½œ
hdfs dfs -mkdir -p /user/data/year=2023/month=12
hdfs dfs -rmr /user/data/old_data

# æŸ¥çœ‹æ–‡ä»¶ä¿¡æ¯
hdfs dfs -stat %b,%o,%r /user/data/file.txt  # å¤§å°ã€å—å¤§å°ã€å‰¯æœ¬æ•°
hdfs fsck /user/data/ -files -blocks -locations  # æ–‡ä»¶ç³»ç»Ÿæ£€æŸ¥

# é›†ç¾¤ç®¡ç†
hdfs dfsadmin -report  # é›†ç¾¤çŠ¶æ€
hdfs dfsadmin -safemode get  # å®‰å…¨æ¨¡å¼çŠ¶æ€
```

### âš™ï¸ æ€§èƒ½è°ƒä¼˜
```xml
<!-- hdfs-site.xml é…ç½®ä¼˜åŒ– -->
<configuration>
    <!-- å—å¤§å°ä¼˜åŒ– -->
    <property>
        <name>dfs.blocksize</name>
        <value>268435456</value>  <!-- 256MBï¼Œé€‚åˆå¤§æ–‡ä»¶ -->
    </property>
    
    <!-- å‰¯æœ¬æ•°è®¾ç½® -->
    <property>
        <name>dfs.replication</name>
        <value>3</value>
    </property>
    
    <!-- NameNodeå†…å­˜ä¼˜åŒ– -->
    <property>
        <name>dfs.namenode.handler.count</name>
        <value>100</value>  <!-- å¤„ç†çº¿ç¨‹æ•° -->
    </property>
</configuration>
```

---

## ğŸ Hive æ•°æ®ä»“åº“

### ğŸ—ï¸ æ¶æ„ç»„ä»¶
```
Hive CLI/Beeline â†’ Hive Server2 â†’ Metastore â†’ HDFS
                     â†“
                 MapReduce/Spark Engine
```

### ğŸ“Š æ•°æ®æ¨¡å‹
- **Database**: æ•°æ®åº“å‘½åç©ºé—´
- **Table**: å¯¹åº”HDFSç›®å½•ç»“æ„
- **Partition**: æ•°æ®åˆ†åŒºï¼Œæé«˜æŸ¥è¯¢æ•ˆç‡
- **Bucket**: æ•°æ®åˆ†æ¡¶ï¼Œä¼˜åŒ–Joinæ“ä½œ

### ğŸ”§ DDLæ“ä½œ
```sql
-- åˆ›å»ºæ•°æ®åº“
CREATE DATABASE IF NOT EXISTS warehouse
COMMENT 'Data warehouse database'
LOCATION '/user/hive/warehouse'
WITH DBPROPERTIES ('creator'='admin');

-- åˆ›å»ºå¤–éƒ¨è¡¨
CREATE EXTERNAL TABLE user_logs (
    user_id BIGINT,
    action STRING,
    timestamp TIMESTAMP,
    ip STRING
)
PARTITIONED BY (
    year INT,
    month INT,
    day INT
)
STORED AS PARQUET
LOCATION '/user/data/logs/';

-- æ·»åŠ åˆ†åŒº
ALTER TABLE user_logs ADD PARTITION (year=2023, month=12, day=25)
LOCATION '/user/data/logs/year=2023/month=12/day=25/';

-- åˆ›å»ºåˆ†æ¡¶è¡¨
CREATE TABLE user_profiles (
    user_id BIGINT,
    name STRING,
    age INT,
    gender STRING
)
CLUSTERED BY (user_id) INTO 32 BUCKETS
STORED AS ORC
TBLPROPERTIES ('transactional'='true');
```

### ğŸ“ˆ æŸ¥è¯¢ä¼˜åŒ–
```sql
-- åˆ†åŒºè£å‰ª
SELECT COUNT(*) FROM user_logs 
WHERE year=2023 AND month=12 AND day=25;

-- åˆ—å¼å­˜å‚¨æŸ¥è¯¢
SELECT user_id, COUNT(*) as action_count
FROM user_logs 
WHERE year=2023 AND month=12
GROUP BY user_id;

-- çª—å£å‡½æ•°
SELECT user_id, action, timestamp,
       ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY timestamp DESC) as rn
FROM user_logs
WHERE year=2023 AND month=12;

-- JOINä¼˜åŒ–
SET hive.auto.convert.join=true;
SET hive.mapjoin.smalltable.filesize=25000000;

SELECT /*+ MAPJOIN(u) */ l.user_id, u.name, COUNT(*) as actions
FROM user_logs l
JOIN user_profiles u ON l.user_id = u.user_id
WHERE l.year=2023 AND l.month=12
GROUP BY l.user_id, u.name;
```

---

## âš¡ Spark ç»Ÿä¸€åˆ†æå¼•æ“

### ğŸ—ï¸ æ ¸å¿ƒç»„ä»¶
```
Spark Application
    â†“
Driver Program â†’ Cluster Manager (YARN/Mesos/K8s)
    â†“              â†“
SparkContext â†’ Executor1, Executor2, Executor3...
```

### ğŸ’» RDDç¼–ç¨‹
```python
from pyspark import SparkContext, SparkConf

# åˆå§‹åŒ–Spark
conf = SparkConf().setAppName("DataProcessing").setMaster("yarn")
sc = SparkContext(conf=conf)

# RDDæ“ä½œ
lines = sc.textFile("hdfs://data/logs.txt")
words = lines.flatMap(lambda line: line.split())
word_counts = words.map(lambda word: (word, 1)) \
                  .reduceByKey(lambda a, b: a + b)

# ç¼“å­˜RDD
word_counts.cache()
word_counts.collect()

# å¹¶è¡Œåº¦è°ƒä¼˜
rdd_optimized = sc.textFile("hdfs://data/large_file.txt", minPartitions=100)
```

### ğŸ“Š DataFrame/Dataset API
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

# åˆ›å»ºSparkSession
spark = SparkSession.builder \
    .appName("DataAnalysis") \
    .config("spark.sql.adaptive.enabled", "true") \
    .getOrCreate()

# è¯»å–æ•°æ®
df = spark.read.format("parquet") \
    .option("path", "hdfs://data/user_events/") \
    .load()

# DataFrameæ“ä½œ
result = df.filter(col("event_date") >= "2023-12-01") \
          .groupBy("user_id", "event_type") \
          .agg(count("*").alias("event_count")) \
          .orderBy(desc("event_count"))

# çª—å£å‡½æ•°
from pyspark.sql.window import Window

window_spec = Window.partitionBy("user_id").orderBy("timestamp")
df_with_rank = df.withColumn("rank", row_number().over(window_spec))

# å†™å…¥æ•°æ®
result.write.format("delta") \
    .mode("overwrite") \
    .option("path", "hdfs://output/user_analysis/") \
    .save()
```

### ğŸš€ æ€§èƒ½ä¼˜åŒ–
```python
# å¹¿æ’­å˜é‡
broadcast_map = spark.sparkContext.broadcast(lookup_dict)

def enrich_data(row):
    return row + (broadcast_map.value.get(row.user_id),)

# ç´¯åŠ å™¨
error_count = spark.sparkContext.accumulator(0)

def process_record(record):
    try:
        # å¤„ç†é€»è¾‘
        return process(record)
    except:
        error_count.add(1)
        return None

# è‡ªé€‚åº”æŸ¥è¯¢æ‰§è¡Œ
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
spark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")
```

---

## ğŸŒŠ Flink æµå¤„ç†å¼•æ“

### ğŸ—ï¸ æ ¸å¿ƒæ¦‚å¿µ
- **DataStream**: æ— ç•Œæ•°æ®æµ
- **Transformation**: æ•°æ®è½¬æ¢æ“ä½œ
- **Sink**: æ•°æ®è¾“å‡º
- **Watermark**: å¤„ç†äº‹ä»¶æ—¶é—´

### ğŸ’» æµå¤„ç†ç¼–ç¨‹
```java
// Java Flinkåº”ç”¨
public class StreamProcessingApp {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        
        // è®¾ç½®æ£€æŸ¥ç‚¹
        env.enableCheckpointing(5000);
        env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);
        
        // æ•°æ®æº
        DataStream<String> source = env.addSource(
            new FlinkKafkaConsumer<>("user-events", new SimpleStringSchema(), properties)
        );
        
        // æ•°æ®è½¬æ¢
        DataStream<UserEvent> events = source
            .map(json -> parseUserEvent(json))
            .filter(event -> event.getEventType().equals("click"))
            .keyBy(UserEvent::getUserId)
            .window(TumblingEventTimeWindows.of(Time.minutes(5)))
            .aggregate(new EventCountAggregator());
        
        // è¾“å‡º
        events.addSink(new FlinkKafkaProducer<>("processed-events", 
                                              new UserEventSerializationSchema(),
                                              properties));
        
        env.execute("User Event Processing");
    }
}
```

### â° æ—¶é—´ä¸çª—å£
```java
// äº‹ä»¶æ—¶é—´å¤„ç†
DataStream<UserEvent> timestampedStream = source
    .assignTimestampsAndWatermarks(
        WatermarkStrategy.<UserEvent>forBoundedOutOfOrderness(Duration.ofSeconds(20))
            .withTimestampAssigner((event, timestamp) -> event.getEventTime())
    );

// ä¸åŒç±»å‹çš„çª—å£
// æ»šåŠ¨çª—å£
.window(TumblingEventTimeWindows.of(Time.minutes(5)))

// æ»‘åŠ¨çª—å£  
.window(SlidingEventTimeWindows.of(Time.minutes(10), Time.minutes(5)))

// ä¼šè¯çª—å£
.window(EventTimeSessionWindows.withGap(Time.minutes(30)))

// è®¡æ•°çª—å£
.countWindow(1000)
```

### ğŸ”„ çŠ¶æ€ç®¡ç†
```java
// é”®æ§çŠ¶æ€
public class StatefulProcessor extends KeyedProcessFunction<String, UserEvent, String> {
    
    private ValueState<Long> countState;
    private MapState<String, Long> featureState;
    
    @Override
    public void open(Configuration parameters) {
        ValueStateDescriptor<Long> countDescriptor = 
            new ValueStateDescriptor<>("count", Long.class, 0L);
        countState = getRuntimeContext().getState(countDescriptor);
        
        MapStateDescriptor<String, Long> featureDescriptor = 
            new MapStateDescriptor<>("features", String.class, Long.class);
        featureState = getRuntimeContext().getMapState(featureDescriptor);
    }
    
    @Override
    public void processElement(UserEvent event, Context ctx, Collector<String> out) 
            throws Exception {
        Long currentCount = countState.value();
        countState.update(currentCount + 1);
        
        featureState.put(event.getFeature(), System.currentTimeMillis());
        
        out.collect("Processed event for user: " + ctx.getCurrentKey());
    }
}
```

---

## ğŸš€ Impala äº¤äº’å¼æŸ¥è¯¢

### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- **MPPæ¶æ„**: å¤§è§„æ¨¡å¹¶è¡Œå¤„ç†
- **å†…å­˜è®¡ç®—**: é¿å…ç£ç›˜I/Oå¼€é”€
- **SQLå…¼å®¹**: æ ‡å‡†SQLè¯­æ³•
- **å®æ—¶æŸ¥è¯¢**: ç§’çº§å“åº”æ—¶é—´

### ğŸ”§ æŸ¥è¯¢ä¼˜åŒ–
```sql
-- åˆ†åŒºè¡¨æŸ¥è¯¢
SELECT customer_id, SUM(amount) as total_amount
FROM sales_fact
WHERE year = 2023 AND month = 12
GROUP BY customer_id;

-- å¤æ‚JOINæŸ¥è¯¢
SELECT /*+ broadcast(d) */ 
    f.product_id, 
    d.product_name,
    SUM(f.sales_amount) as total_sales
FROM sales_fact f
JOIN product_dim d ON f.product_id = d.product_id
WHERE f.year = 2023
GROUP BY f.product_id, d.product_name
ORDER BY total_sales DESC
LIMIT 100;

-- çª—å£å‡½æ•°
SELECT 
    customer_id,
    order_date,
    amount,
    SUM(amount) OVER (
        PARTITION BY customer_id 
        ORDER BY order_date 
        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
    ) as running_total
FROM orders
WHERE year = 2023;
```

### âš™ï¸ æ€§èƒ½è°ƒä¼˜
```sql
-- ç»Ÿè®¡ä¿¡æ¯æ›´æ–°
COMPUTE STATS sales_fact;
COMPUTE INCREMENTAL STATS sales_fact PARTITION (year=2023, month=12);

-- æŸ¥è¯¢æç¤º
SELECT /*+ straight_join */ *
FROM large_table a
JOIN small_table b ON a.id = b.id;

-- å†…å­˜ä¼˜åŒ–
SET mem_limit=8GB;
SET disable_codegen=false;
SET runtime_filter_mode=GLOBAL;
```

---

## ğŸº HBase åˆ—æ—æ•°æ®åº“

### ğŸ—ï¸ æ•°æ®æ¨¡å‹
```
Table
  â†“
Row Key â†’ Column Family â†’ Column Qualifier â†’ Cell (Value + Timestamp)
```

### ğŸ’» Java APIæ“ä½œ
```java
// è¿æ¥HBase
Configuration conf = HBaseConfiguration.create();
conf.set("hbase.zookeeper.quorum", "zk1,zk2,zk3");
Connection connection = ConnectionFactory.createConnection(conf);

// åˆ›å»ºè¡¨
Admin admin = connection.getAdmin();
TableName tableName = TableName.valueOf("user_profiles");
HTableDescriptor tableDesc = new HTableDescriptor(tableName);
tableDesc.addFamily(new HColumnDescriptor("info"));
tableDesc.addFamily(new HColumnDescriptor("stats"));
admin.createTable(tableDesc);

// æ’å…¥æ•°æ®
Table table = connection.getTable(tableName);
Put put = new Put(Bytes.toBytes("user123"));
put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("name"), Bytes.toBytes("John"));
put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("age"), Bytes.toBytes("30"));
put.addColumn(Bytes.toBytes("stats"), Bytes.toBytes("login_count"), Bytes.toBytes("150"));
table.put(put);

// æŸ¥è¯¢æ•°æ®
Get get = new Get(Bytes.toBytes("user123"));
Result result = table.get(get);
byte[] name = result.getValue(Bytes.toBytes("info"), Bytes.toBytes("name"));
System.out.println("Name: " + Bytes.toString(name));

// æ‰«ææ•°æ®
Scan scan = new Scan();
scan.setStartRow(Bytes.toBytes("user100"));
scan.setStopRow(Bytes.toBytes("user200"));
ResultScanner scanner = table.getScanner(scan);
for (Result r : scanner) {
    // å¤„ç†ç»“æœ
    processResult(r);
}
```

### ğŸ”§ æ€§èƒ½ä¼˜åŒ–
```java
// æ‰¹é‡æ“ä½œ
List<Put> puts = new ArrayList<>();
for (UserData user : users) {
    Put put = new Put(Bytes.toBytes(user.getId()));
    put.addColumn(Bytes.toBytes("info"), Bytes.toBytes("name"), 
                  Bytes.toBytes(user.getName()));
    puts.add(put);
}
table.put(puts);

// é¢„åˆ†åŒº
byte[][] splitKeys = new byte[][] {
    Bytes.toBytes("1000"),
    Bytes.toBytes("2000"),
    Bytes.toBytes("3000")
};
admin.createTable(tableDesc, splitKeys);

// å¸ƒéš†è¿‡æ»¤å™¨
HColumnDescriptor cf = new HColumnDescriptor("info");
cf.setBloomFilterType(BloomType.ROW);
cf.setBlockCacheEnabled(true);
tableDesc.addFamily(cf);
```

---

## ğŸ¯ å¸¸è§é¢è¯•é¢˜åŠç­”æ¡ˆ

### Q1: HDFSçš„è¯»å†™æµç¨‹æ˜¯ä»€ä¹ˆï¼Ÿ
**A**: 
**å†™æµç¨‹**:
1. Clientå‘NameNodeè¯·æ±‚å†™æ–‡ä»¶
2. NameNodeæ£€æŸ¥æƒé™å’Œå‘½åç©ºé—´ï¼Œè¿”å›DataNodeåˆ—è¡¨
3. Clientå‘ç¬¬ä¸€ä¸ªDataNodeå†™æ•°æ®
4. DataNodeä¹‹é—´å½¢æˆPipelineè¿›è¡Œå‰¯æœ¬å¤åˆ¶
5. æ‰€æœ‰å‰¯æœ¬å†™å…¥å®Œæˆåè¿”å›ç¡®è®¤

**è¯»æµç¨‹**:
1. Clientå‘NameNodeè¯·æ±‚è¯»æ–‡ä»¶
2. NameNodeè¿”å›æ–‡ä»¶å—ä½ç½®ä¿¡æ¯
3. Clientç›´æ¥ä»DataNodeè¯»å–æ•°æ®
4. é€‰æ‹©æœ€è¿‘çš„DataNodeè¯»å–ä»¥ä¼˜åŒ–æ€§èƒ½

### Q2: Hiveå’Œä¼ ç»Ÿæ•°æ®åº“çš„åŒºåˆ«ï¼Ÿ
**A**: 
- **æ•°æ®è§„æ¨¡**: Hiveå¤„ç†PBçº§æ•°æ®ï¼Œä¼ ç»Ÿæ•°æ®åº“å¤„ç†GB-TBçº§
- **æŸ¥è¯¢å»¶è¿Ÿ**: Hiveåˆ†é’Ÿçº§ï¼Œä¼ ç»Ÿæ•°æ®åº“æ¯«ç§’-ç§’çº§
- **ACIDæ”¯æŒ**: Hiveéƒ¨åˆ†æ”¯æŒï¼Œä¼ ç»Ÿæ•°æ®åº“å®Œå…¨æ”¯æŒ
- **Schema**: Hiveè¯»æ—¶æ¨¡å¼ï¼Œä¼ ç»Ÿæ•°æ®åº“å†™æ—¶æ¨¡å¼
- **è®¡ç®—æ¨¡å¼**: Hiveæ‰¹å¤„ç†ï¼Œä¼ ç»Ÿæ•°æ®åº“äº‹åŠ¡å¤„ç†

### Q3: Sparkç›¸æ¯”MapReduceçš„ä¼˜åŠ¿ï¼Ÿ
**A**: 
- **å†…å­˜è®¡ç®—**: SparkåŸºäºå†…å­˜ï¼ŒMapReduceåŸºäºç£ç›˜
- **DAGæ‰§è¡Œ**: Sparkæ”¯æŒå¤æ‚æ•°æ®æµï¼ŒMapReduceåªæ”¯æŒMap-Reduce
- **ç¼–ç¨‹æ¨¡å‹**: Sparkæä¾›ä¸°å¯ŒAPIï¼ŒMapReduceç¼–ç¨‹å¤æ‚
- **è¿­ä»£ç®—æ³•**: Sparkå¤©ç„¶æ”¯æŒï¼ŒMapReduceéœ€è¦å¤šè½®ä½œä¸š
- **å®æ—¶å¤„ç†**: Sparkæ”¯æŒæµå¤„ç†ï¼ŒMapReduceåªæ”¯æŒæ‰¹å¤„ç†

### Q4: Flinkçš„Exactly-Onceè¯­ä¹‰å¦‚ä½•å®ç°ï¼Ÿ
**A**: 
1. **æ£€æŸ¥ç‚¹æœºåˆ¶**: å®šæœŸä¿å­˜ç®—å­çŠ¶æ€å¿«ç…§
2. **ä¸¤é˜¶æ®µæäº¤**: å¯¹å¤–éƒ¨ç³»ç»Ÿä½¿ç”¨2PCåè®®
3. **å¹‚ç­‰å†™å…¥**: æ”¯æŒå¹‚ç­‰çš„Sinkè¿æ¥å™¨
4. **çŠ¶æ€æ¢å¤**: å¤±è´¥æ—¶ä»æœ€è¿‘æ£€æŸ¥ç‚¹æ¢å¤
5. **ç«¯åˆ°ç«¯ä¿è¯**: ä»Sourceåˆ°Sinkçš„å®Œæ•´ä¿è¯

### Q5: HBaseé€‚åˆä»€ä¹ˆåœºæ™¯ï¼Ÿä¸é€‚åˆä»€ä¹ˆåœºæ™¯ï¼Ÿ
**A**: 
**é€‚åˆåœºæ™¯**:
- å¤§æ•°æ®é‡çš„éšæœºè¯»å†™
- ç¨€ç–æ•°æ®å­˜å‚¨
- å®æ—¶æ•°æ®è®¿é—®
- æ—¶åºæ•°æ®å­˜å‚¨

**ä¸é€‚åˆåœºæ™¯**:
- å¤æ‚çš„å…³è”æŸ¥è¯¢
- å°æ•°æ®é‡åº”ç”¨
- å¼ºä¸€è‡´æ€§è¦æ±‚
- é¢‘ç¹çš„å…¨è¡¨æ‰«æ

### Q6: å¦‚ä½•ä¼˜åŒ–å¤§æ•°æ®æŸ¥è¯¢æ€§èƒ½ï¼Ÿ
**A**: 
1. **åˆ†åŒºè®¾è®¡**: åˆç†çš„åˆ†åŒºç­–ç•¥å‡å°‘æ‰«ææ•°æ®é‡
2. **ç´¢å¼•ä¼˜åŒ–**: åˆ›å»ºåˆé€‚çš„ç´¢å¼•åŠ é€ŸæŸ¥è¯¢
3. **æ•°æ®æ ¼å¼**: ä½¿ç”¨åˆ—å¼å­˜å‚¨æ ¼å¼(Parquetã€ORC)
4. **æ•°æ®å‹ç¼©**: å‡å°‘I/Oå’Œå­˜å‚¨å¼€é”€
5. **æŸ¥è¯¢ä¼˜åŒ–**: è°“è¯ä¸‹æ¨ã€åˆ—è£å‰ªã€JOINä¼˜åŒ–
6. **ç¼“å­˜ç­–ç•¥**: åˆç†ä½¿ç”¨å†…å­˜ç¼“å­˜
7. **å¹¶è¡Œåº¦è°ƒä¼˜**: è°ƒæ•´ä»»åŠ¡å¹¶è¡Œåº¦

### Q7: å¤§æ•°æ®æ¶æ„è®¾è®¡éœ€è¦è€ƒè™‘å“ªäº›å› ç´ ï¼Ÿ
**A**: 
- **æ•°æ®ç‰¹æ€§**: æ•°æ®é‡ã€å¢é•¿é€Ÿåº¦ã€æ•°æ®ç±»å‹
- **ä¸šåŠ¡éœ€æ±‚**: å®æ—¶æ€§è¦æ±‚ã€æŸ¥è¯¢æ¨¡å¼ã€SLA
- **æŠ€æœ¯é€‰å‹**: æ‰¹å¤„ç†vsæµå¤„ç†ã€SQL vs NoSQL
- **æˆæœ¬è€ƒè™‘**: ç¡¬ä»¶æˆæœ¬ã€è¿ç»´æˆæœ¬ã€äººå‘˜æˆæœ¬
- **æ‰©å±•æ€§**: æ°´å¹³æ‰©å±•èƒ½åŠ›ã€å­˜å‚¨æ‰©å±•èƒ½åŠ›
- **å¯é æ€§**: æ•°æ®å®¹é”™ã€æœåŠ¡å¯ç”¨æ€§
- **å®‰å…¨æ€§**: æ•°æ®åŠ å¯†ã€è®¿é—®æ§åˆ¶ã€å®¡è®¡

### Q8: Lambdaæ¶æ„å’ŒKappaæ¶æ„çš„åŒºåˆ«ï¼Ÿ
**A**: 
**Lambdaæ¶æ„**:
- æ‰¹å¤„ç†å±‚ + æµå¤„ç†å±‚ + æœåŠ¡å±‚
- æ•°æ®åŒå†™ï¼Œä¿è¯å‡†ç¡®æ€§
- å¤æ‚åº¦é«˜ï¼Œç»´æŠ¤æˆæœ¬å¤§

**Kappaæ¶æ„**:
- åªæœ‰æµå¤„ç†å±‚
- æ‰€æœ‰æ•°æ®å½“ä½œæµå¤„ç†
- æ¶æ„ç®€å•ï¼Œç»´æŠ¤å®¹æ˜“
- ä¾èµ–æµå¤„ç†å¼•æ“çš„å¯é æ€§

### Q9: æ•°æ®æ¹–å’Œæ•°æ®ä»“åº“çš„åŒºåˆ«ï¼Ÿ
**A**: 
| ç‰¹æ€§ | æ•°æ®æ¹– | æ•°æ®ä»“åº“ |
|:---|:---|:---|
| **æ•°æ®ç»“æ„** | åŸå§‹æ•°æ®ï¼Œç»“æ„åŒ–+éç»“æ„åŒ– | ç»“æ„åŒ–æ•°æ® |
| **Schema** | Schema-on-Read | Schema-on-Write |
| **å­˜å‚¨æˆæœ¬** | ä½ | é«˜ |
| **å¤„ç†é€Ÿåº¦** | æ…¢ | å¿« |
| **çµæ´»æ€§** | é«˜ | ä½ |
| **æ•°æ®è´¨é‡** | ä½ | é«˜ |

### Q10: å¦‚ä½•ä¿è¯å¤§æ•°æ®å¹³å°çš„æ•°æ®è´¨é‡ï¼Ÿ
**A**: 
1. **æ•°æ®é‡‡é›†**: æºå¤´æ•°æ®æ ¡éªŒå’Œæ¸…æ´—
2. **ETLè¿‡ç¨‹**: æ•°æ®è½¬æ¢è¿‡ç¨‹ä¸­çš„è´¨é‡æ£€æŸ¥
3. **æ•°æ®ç›‘æ§**: å®æ—¶ç›‘æ§æ•°æ®è´¨é‡æŒ‡æ ‡
4. **æ•°æ®è¡€ç¼˜**: è·Ÿè¸ªæ•°æ®æ¥æºå’Œå¤„ç†è¿‡ç¨‹
5. **å¼‚å¸¸æ£€æµ‹**: è‡ªåŠ¨æ£€æµ‹æ•°æ®å¼‚å¸¸å’Œæ³¢åŠ¨
6. **æ•°æ®æ²»ç†**: å»ºç«‹æ•°æ®è´¨é‡ç®¡ç†åˆ¶åº¦
7. **å…ƒæ•°æ®ç®¡ç†**: å®Œå–„çš„å…ƒæ•°æ®ç®¡ç†ä½“ç³»