# åˆ†å¸ƒå¼çˆ¬è™«å®æˆ˜ï¼šScrapy-Redis è¯¦è§£

> **ğŸ“š å‰ç½®çŸ¥è¯†**
>
> æœ¬é…æ–¹æ¶‰åŠä»¥ä¸‹æ ¸å¿ƒæŠ€æœ¯ï¼Œå»ºè®®å…ˆé˜…è¯»ç›¸å…³ç« èŠ‚ï¼š
>
> - **[Scrapy å¿«é€Ÿå…¥é—¨](./scrapy.md)** - ç†è§£ Scrapy è°ƒåº¦å™¨ä¸å»é‡æœºåˆ¶
> - **Redis åŸºç¡€** - æŒæ¡ Redis åˆ—è¡¨ã€é›†åˆæ“ä½œ

Scrapy é»˜è®¤æ˜¯å•æœºæ¶æ„ï¼Œè¯·æ±‚é˜Ÿåˆ—ä¿å­˜åœ¨å†…å­˜ä¸­ï¼Œé‡å¯å³å¤±ï¼Œä¸”æ— æ³•å¤šæœºå…±äº«ã€‚**Scrapy-Redis** æ˜¯ä¸€ä¸ªå¼ºå¤§çš„ç»„ä»¶ï¼Œå®ƒé‡å†™äº† Scrapy çš„è°ƒåº¦å™¨ (Scheduler) å’Œå»é‡ç»„ä»¶ (DupeFilter)ï¼Œå°†è¯·æ±‚é˜Ÿåˆ—å’ŒæŒ‡çº¹é›†åˆå­˜å‚¨åœ¨ Redis ä¸­ï¼Œä»è€Œå®ç°ï¼š

1. **åˆ†å¸ƒå¼çˆ¬å–**: å¤šä¸ªçˆ¬è™«èŠ‚ç‚¹å…±äº«åŒä¸€ä¸ª Redis é˜Ÿåˆ—ï¼ŒååŒå·¥ä½œã€‚
2. **æ–­ç‚¹ç»­çˆ¬**: è¯·æ±‚æŒä¹…åŒ–åœ¨ Redis ä¸­ï¼Œçˆ¬è™«æŒ‚æ‰é‡å¯åå¯ç»§ç»­è¿è¡Œã€‚

---

## 1. æ ¸å¿ƒæ¶æ„åŸç†

### åŸç”Ÿ Scrapy vs Scrapy-Redis

**åŸç”Ÿ Scrapy**:

- **Scheduler**: ç»´æŠ¤åœ¨å†…å­˜ä¸­çš„ Python `deque` æˆ– `queue`ã€‚
- **DupeFilter**: ç»´æŠ¤åœ¨å†…å­˜ä¸­çš„ Python `set`ã€‚
- **ç¼ºç‚¹**: æ— æ³•è·¨è¿›ç¨‹/è·¨æœºå™¨å…±äº«ï¼Œå†…å­˜å—é™ã€‚

**Scrapy-Redis**:

- **Scheduler**: ä» Redis çš„ `List` (æˆ– `PriorityQueue`) ä¸­ `POP` è¯·æ±‚ï¼Œå‘å…¶ `PUSH` æ–°è¯·æ±‚ã€‚
- **DupeFilter**: åˆ©ç”¨ Redis çš„ `Set` æ•°æ®ç»“æ„å­˜å‚¨ URL æŒ‡çº¹ (SHA1)ï¼Œåˆ©ç”¨ Redis çš„åŸå­æ€§è¿›è¡Œå»é‡ã€‚
- **Item Pipeline**: å¯é€‰å°†æå–çš„æ•°æ®ç›´æ¥æ¨å…¥ Redisï¼Œç”±ç‹¬ç«‹çš„ Worker æ¶ˆè´¹å­˜å‚¨ã€‚

---

## 2. ç¯å¢ƒæ­å»ºä¸é…ç½®

### å®‰è£…

```bash
pip install scrapy-redis
```

### é…ç½® settings.py

```python
# settings.py

# 1. å¯ç”¨ Scrapy-Redis è°ƒåº¦å™¨
SCHEDULER = "scrapy_redis.scheduler.Scheduler"

# 2. å¯ç”¨ Scrapy-Redis å»é‡è¿‡æ»¤å™¨
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"

# 3. å…è®¸æš‚åœ (æ–­ç‚¹ç»­çˆ¬æ ¸å¿ƒ)
# å¦‚æœä¸ºçœŸï¼Œå½“çˆ¬è™«åœæ­¢æ—¶ï¼ŒRedis ä¸­çš„è¯·æ±‚é˜Ÿåˆ—ä¸ä¼šè¢«æ¸…ç©º
SCHEDULER_PERSIST = True

# 4. è®¾ç½® Redis è¿æ¥
# æ–¹å¼ä¸€ï¼šå•ç‹¬è®¾ç½®
REDIS_HOST = '192.168.1.100'
REDIS_PORT = 6379
# REDIS_PARAMS = {'password': 'yourpassword'}

# æ–¹å¼äºŒï¼šå®Œæ•´åœ°å€
# REDIS_URL = 'redis://user:pass@hostname:9001'

# 5. é…ç½®è¯·æ±‚é˜Ÿåˆ—æ¨¡å¼ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸º PriorityQueueï¼‰
# SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.PriorityQueue'  # æœ‰åºé›†åˆï¼Œæ”¯æŒä¼˜å…ˆçº§
# SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.FifoQueue'      # å…ˆè¿›å…ˆå‡ºåˆ—è¡¨
# SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.LifoQueue'      # åè¿›å…ˆå‡ºåˆ—è¡¨ï¼ˆæ ˆï¼‰

# 6. (å¯é€‰) å°†æ•°æ®å­˜å…¥ Redis Pipeline
ITEM_PIPELINES = {
    'scrapy_redis.pipelines.RedisPipeline': 300,
}
```

---

## 3. ç¼–å†™åˆ†å¸ƒå¼ Spider

### ç»§æ‰¿ RedisSpider

```python
from scrapy_redis.spiders import RedisSpider
import scrapy

class MyDistributedSpider(RedisSpider):
    name = 'myspider_distributed'

    # æ ¸å¿ƒå·®å¼‚ï¼šä¸å†å®šä¹‰ start_urls
    # å®šä¹‰ redis_keyï¼Œçˆ¬è™«å¯åŠ¨åä¼šé˜»å¡ç­‰å¾…è¯¥é”®ä¸­å‡ºç°çš„ URL
    redis_key = 'myspider:start_urls'

    def parse(self, response):
        self.logger.info(f"Crawling {response.url}")

        # æå–æ•°æ®é€»è¾‘ä¸æ™®é€š Spider ä¸€è‡´
        yield {
            'url': response.url,
            'title': response.css('title::text').get()
        }

        # ç”Ÿæˆæ–°è¯·æ±‚
        for href in response.css('a::attr(href)').getall():
            yield response.follow(href, self.parse)
```

### å¯åŠ¨æµç¨‹

1. å¯åŠ¨çˆ¬è™«ï¼ˆå¤šä¸ªç»ˆç«¯å¯åŠ¨å¤šä¸ªå®ä¾‹ï¼‰ï¼š
    ```bash
    scrapy crawl myspider_distributed
    ```
2. å‘ Redis æ¨é€èµ·å§‹ URLï¼š
    ```bash
    redis-cli lpush myspider:start_urls http://example.com
    ```

### ä½¿ç”¨ RedisCrawlSpider

å¦‚æœä½ éœ€è¦åˆ©ç”¨ `Rule` å’Œ `LinkExtractor` è‡ªåŠ¨æŠ“å–å…¨ç«™ï¼Œå¯ä»¥ä½¿ç”¨ `RedisCrawlSpider`ã€‚

```python
from scrapy_redis.spiders import RedisCrawlSpider
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import Rule

class MyCrawlSpider(RedisCrawlSpider):
    name = 'mycrawl_distributed'
    redis_key = 'mycrawl:start_urls'

    rules = (
        Rule(LinkExtractor(allow=r'/article/'), callback='parse_article'),
        Rule(LinkExtractor(allow=r'/page/'), follow=True),
    )

    def parse_article(self, response):
        yield {
            'title': response.css('h1::text').get(),
            'content': response.css('.content::text').getall(),
        }
```

---

## 4. è¿›é˜¶ä¼˜åŒ–ç­–ç•¥

### Bloom Filter å»é‡ä¼˜åŒ–

Scrapy-Redis é»˜è®¤ä½¿ç”¨ Redis `Set` å­˜å‚¨æ‰€æœ‰æŒ‡çº¹ã€‚å¯¹äºäº¿çº§ URL çš„çˆ¬å–ï¼Œè¿™ä¼šæ¶ˆè€—æ•°å GB å†…å­˜ã€‚è§£å†³æ–¹æ¡ˆæ˜¯é›†æˆ **Bloom Filter**ã€‚

**å®ç°æ€è·¯**:

1. é‡å†™ `RFPDupeFilter`ã€‚
2. ä½¿ç”¨ `redis-py` çš„ `bf.add` å’Œ `bf.exists` å‘½ä»¤ (éœ€è¦ RedisBloom æ¨¡å—) æˆ– Python ç«¯çš„ `pybloom_live` æ˜ å°„åˆ° Redis BitMapã€‚

```python
# custom_dupefilter.py ç®€æ˜“ç¤ºæ„
from scrapy_redis.dupefilter import RFPDupeFilter

class BloomFilterDupeFilter(RFPDupeFilter):
    def request_seen(self, request):
        fp = self.request_fingerprint(request)
        # å‡è®¾ self.server æ˜¯ Redis è¿æ¥ï¼Œä¸”æ”¯æŒ BF å‘½ä»¤
        # å¦‚æœæŒ‡çº¹å·²å­˜åœ¨ï¼Œè¿”å› True
        if self.server.execute_command('BF.EXISTS', self.key, fp):
            return True
        # å¦åˆ™æ·»åŠ æŒ‡çº¹
        self.server.execute_command('BF.ADD', self.key, fp)
        return False
```

### è¯·æ±‚ä¼˜å…ˆçº§

```python
# ç”Ÿæˆè¯·æ±‚æ—¶æŒ‡å®šä¼˜å…ˆçº§
yield scrapy.Request(url, priority=100)  # é¦–é¡µï¼Œä¼˜å…ˆ
yield scrapy.Request(url, priority=10)   # è¯¦æƒ…é¡µï¼Œæ¬¡ä¹‹
```

### ç©ºé—²è¶…æ—¶å…³é—­

```python
# settings.py
# æŒ‡å®šç©ºé—²ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œè¶…æ—¶åˆ™å…³é—­çˆ¬è™«
SCHEDULER_IDLE_BEFORE_CLOSE = 10
```

---

## 5. éƒ¨ç½²æ¶æ„å›¾

```mermaid
graph TB
    subgraph "Master / Redis Server"
        Redis[(Redis Queue & Set)]
    end

    subgraph "Slave 1"
        Spider1[Scrapy Spider 1] -->|Pop Request| Redis
        Spider1 -->|Push Request| Redis
        Spider1 -->|Dupe Check| Redis
    end

    subgraph "Slave 2"
        Spider2[Scrapy Spider 2] -->|Pop Request| Redis
        Spider2 -->|Push Request| Redis
        Spider2 -->|Dupe Check| Redis
    end

    subgraph "Data Storage"
        Mongo[(MongoDB)]
    end

    Spider1 -->|Store Item| Mongo
    Spider2 -->|Store Item| Mongo
```

---

## 6. å¸¸è§é—®é¢˜

### Q: çˆ¬è™«å¯åŠ¨åä¸€ç›´ç­‰å¾…ï¼Œä¸æŠ“å–ï¼Ÿ

**A**: ç¡®è®¤å·²å‘ `redis_key` æ¨é€äº†èµ·å§‹ URLï¼š

```bash
redis-cli lpush myspider:start_urls http://example.com
```

### Q: å¦‚ä½•ç›‘æ§çˆ¬å–è¿›åº¦ï¼Ÿ

**A**: ä½¿ç”¨ Redis å‘½ä»¤æŸ¥çœ‹é˜Ÿåˆ—é•¿åº¦ï¼š

```bash
# æŸ¥çœ‹å¾…çˆ¬å–è¯·æ±‚æ•°é‡
redis-cli llen myspider:requests

# æŸ¥çœ‹å·²çˆ¬å–æŒ‡çº¹æ•°é‡
redis-cli scard myspider:dupefilter
```

### Q: å¦‚ä½•æ¸…ç©ºé˜Ÿåˆ—é‡æ–°å¼€å§‹ï¼Ÿ

**A**: åˆ é™¤ Redis ä¸­çš„ç›¸å…³é”®ï¼š

```bash
redis-cli del myspider:requests myspider:dupefilter myspider:start_urls
```

---

## æ€»ç»“

Scrapy-Redis æ˜¯æ„å»ºåˆ†å¸ƒå¼çˆ¬è™«ç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ã€‚é€šè¿‡å°†è°ƒåº¦å™¨å’Œå»é‡ç»„ä»¶è¿ç§»åˆ° Redisï¼Œå®ç°äº†å¤šèŠ‚ç‚¹ååŒçˆ¬å–å’Œæ–­ç‚¹ç»­çˆ¬èƒ½åŠ›ï¼Œæ˜¯å¤§è§„æ¨¡æ•°æ®é‡‡é›†çš„å¿…å¤‡å·¥å…·ã€‚
